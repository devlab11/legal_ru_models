{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LegalRuModels.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPq3zeSNLgA+eczLKFDeVAW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/devlab11/legal_ru_models/blob/main/LegalRuModels.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HE3ET2XW9GRH"
      },
      "source": [
        "**Copyright 2020 Mitlabs.**\n",
        "\n",
        "Licensed under the Apache License, Version 2.0 (the \"License\");"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1gHJ8GBc99GF"
      },
      "source": [
        "# LegalRuModels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gryfKbHd-Eu7"
      },
      "source": [
        "Фемида и трансформеры:\n",
        "использование лингвистических моделей на основе архитектуры “трансформер” в юридической практике. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MfBg1C5NB3X0"
      },
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  \n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/drive/1x2R3HjqFgYMDMp7uYMNm9tkBf9u4WkXR?authuser=1#scrollTo=MfBg1C5NB3X0\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://github.com/devlab11/legal_ru_models/blob/main/LegalRuModels.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View on GitHub</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a href=\"https://colab.research.google.com/drive/1x2R3HjqFgYMDMp7uYMNm9tkBf9u4WkXR?usp=sharing\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\" />Download notebook</a>\n",
        "  </td>\n",
        "  \n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IpxFIba8Gikk"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TSYZZrFOGh2q",
        "outputId": "03333a03-7fed-4621-9864-7bc129ae050e"
      },
      "source": [
        "!pip install transformers\n",
        "!pip install tensorflow_text\n",
        "!pip install bokeh\n",
        "!pip install simpleneighbors[annoy]\n",
        "!pip install tqdm"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/99/84/7bc03215279f603125d844bf81c3fb3f2d50fe8e511546eb4897e4be2067/transformers-4.0.0-py3-none-any.whl (1.4MB)\n",
            "\u001b[K     |████████████████████████████████| 1.4MB 13.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n",
            "Collecting tokenizers==0.9.4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0f/1c/e789a8b12e28be5bc1ce2156cf87cb522b379be9cadc7ad8091a4cc107c4/tokenizers-0.9.4-cp36-cp36m-manylinux2010_x86_64.whl (2.9MB)\n",
            "\u001b[K     |████████████████████████████████| 2.9MB 52.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.8)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 28.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (1.15.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.11.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.17.0)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893257 sha256=0c65e376474448703042da033e8db12f8a8946bb9241c687ffd107df27ab5e19\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: tokenizers, sacremoses, transformers\n",
            "Successfully installed sacremoses-0.0.43 tokenizers-0.9.4 transformers-4.0.0\n",
            "Collecting tensorflow_text\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/28/b2/2dbd90b93913afd07e6101b8b84327c401c394e60141c1e98590038060b3/tensorflow_text-2.3.0-cp36-cp36m-manylinux1_x86_64.whl (2.6MB)\n",
            "\u001b[K     |████████████████████████████████| 2.6MB 13.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: tensorflow<2.4,>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow_text) (2.3.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.4,>=2.3.0->tensorflow_text) (3.3.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.4,>=2.3.0->tensorflow_text) (1.12.1)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.4,>=2.3.0->tensorflow_text) (1.1.0)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.4,>=2.3.0->tensorflow_text) (3.12.4)\n",
            "Requirement already satisfied: tensorflow-estimator<2.4.0,>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.4,>=2.3.0->tensorflow_text) (2.3.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.4,>=2.3.0->tensorflow_text) (1.15.0)\n",
            "Requirement already satisfied: keras-preprocessing<1.2,>=1.1.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.4,>=2.3.0->tensorflow_text) (1.1.2)\n",
            "Requirement already satisfied: h5py<2.11.0,>=2.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.4,>=2.3.0->tensorflow_text) (2.10.0)\n",
            "Requirement already satisfied: numpy<1.19.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.4,>=2.3.0->tensorflow_text) (1.18.5)\n",
            "Requirement already satisfied: tensorboard<3,>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.4,>=2.3.0->tensorflow_text) (2.3.0)\n",
            "Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.4,>=2.3.0->tensorflow_text) (0.3.3)\n",
            "Requirement already satisfied: astunparse==1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.4,>=2.3.0->tensorflow_text) (1.6.3)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.4,>=2.3.0->tensorflow_text) (1.33.2)\n",
            "Requirement already satisfied: scipy==1.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.4,>=2.3.0->tensorflow_text) (1.4.1)\n",
            "Requirement already satisfied: google-pasta>=0.1.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.4,>=2.3.0->tensorflow_text) (0.2.0)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.4,>=2.3.0->tensorflow_text) (0.10.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.4,>=2.3.0->tensorflow_text) (0.35.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.9.2->tensorflow<2.4,>=2.3.0->tensorflow_text) (50.3.2)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow<2.4,>=2.3.0->tensorflow_text) (0.4.2)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow<2.4,>=2.3.0->tensorflow_text) (1.7.0)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow<2.4,>=2.3.0->tensorflow_text) (2.23.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow<2.4,>=2.3.0->tensorflow_text) (3.3.3)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow<2.4,>=2.3.0->tensorflow_text) (1.0.1)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow<2.4,>=2.3.0->tensorflow_text) (1.17.2)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow<2.4,>=2.3.0->tensorflow_text) (1.3.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow<2.4,>=2.3.0->tensorflow_text) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow<2.4,>=2.3.0->tensorflow_text) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow<2.4,>=2.3.0->tensorflow_text) (2020.11.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow<2.4,>=2.3.0->tensorflow_text) (1.24.3)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow<2.4,>=2.3.0->tensorflow_text) (2.0.0)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow<2.4,>=2.3.0->tensorflow_text) (4.1.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow<2.4,>=2.3.0->tensorflow_text) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow<2.4,>=2.3.0->tensorflow_text) (4.6)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow<2.4,>=2.3.0->tensorflow_text) (3.1.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow<2.4,>=2.3.0->tensorflow_text) (3.4.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.6/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow<2.4,>=2.3.0->tensorflow_text) (0.4.8)\n",
            "Installing collected packages: tensorflow-text\n",
            "Successfully installed tensorflow-text-2.3.0\n",
            "Requirement already satisfied: bokeh in /usr/local/lib/python3.6/dist-packages (2.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from bokeh) (2.8.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.6/dist-packages (from bokeh) (3.7.4.3)\n",
            "Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.6/dist-packages (from bokeh) (1.18.5)\n",
            "Requirement already satisfied: tornado>=5.1 in /usr/local/lib/python3.6/dist-packages (from bokeh) (5.1.1)\n",
            "Requirement already satisfied: PyYAML>=3.10 in /usr/local/lib/python3.6/dist-packages (from bokeh) (3.13)\n",
            "Requirement already satisfied: packaging>=16.8 in /usr/local/lib/python3.6/dist-packages (from bokeh) (20.4)\n",
            "Requirement already satisfied: Jinja2>=2.7 in /usr/local/lib/python3.6/dist-packages (from bokeh) (2.11.2)\n",
            "Requirement already satisfied: pillow>=4.0 in /usr/local/lib/python3.6/dist-packages (from bokeh) (7.0.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.1->bokeh) (1.15.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging>=16.8->bokeh) (2.4.7)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from Jinja2>=2.7->bokeh) (1.1.1)\n",
            "Collecting simpleneighbors[annoy]\n",
            "  Downloading https://files.pythonhosted.org/packages/f9/10/9092e15d9aa4a9e5a263416121f124e565766767e7866e11d7074ec50df5/simpleneighbors-0.1.0-py2.py3-none-any.whl\n",
            "Collecting annoy>=1.16.0; extra == \"annoy\"\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a1/5b/1c22129f608b3f438713b91cd880dc681d747a860afe3e8e0af86e921942/annoy-1.17.0.tar.gz (646kB)\n",
            "\u001b[K     |████████████████████████████████| 655kB 15.9MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: annoy\n",
            "  Building wheel for annoy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for annoy: filename=annoy-1.17.0-cp36-cp36m-linux_x86_64.whl size=390347 sha256=5d2e9b54c9d55686bc19bd1c4aa6577555afe1d7ee50e97d9075b2c45b3db7c6\n",
            "  Stored in directory: /root/.cache/pip/wheels/3a/c5/59/cce7e67b52c8e987389e53f917b6bb2a9d904a03246fadcb1e\n",
            "Successfully built annoy\n",
            "Installing collected packages: annoy, simpleneighbors\n",
            "Successfully installed annoy-1.17.0 simpleneighbors-0.1.0\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (4.41.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T1ygvZ_3G4xO",
        "outputId": "4f847b40-e17a-4f37-942a-b4392bc4d2b7"
      },
      "source": [
        "import pickle\n",
        "import time\n",
        "import warnings\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import transformers as ppb\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "import bz2\n",
        "import csv\n",
        "import pickle\n",
        "\n",
        "import _pickle as cPickle\n",
        "from numpy import savetxt\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "from transformers import BertTokenizer, BertModel\n",
        "from transformers import AutoModel,AutoTokenizer, AutoModelWithLMHead\n",
        "from transformers import LongformerModel, LongformerTokenizer\n",
        "from transformers import (ElectraConfig, ElectraForMaskedLM,\n",
        "                          ElectraForPreTraining, ElectraForTokenClassification,\n",
        "                          ElectraModel, ElectraTokenizerFast)\n",
        "import pickle\n",
        "import bz2\n",
        "import _pickle as cPickle\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import gc\n",
        "from numpy import loadtxt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.preprocessing import normalize\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib as plt\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import cross_validate\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "import time\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.gaussian_process import GaussianProcessClassifier\n",
        "from sklearn.gaussian_process.kernels import RBF\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import make_scorer, accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, auc, roc_curve\n",
        "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.feature_selection import SelectFromModel\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "from sklearn.preprocessing import label_binarize\n",
        "import numpy as np\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import precision_recall_curve\n",
        "from sklearn.metrics import plot_precision_recall_curve\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from sklearn.metrics import precision_recall_curve\n",
        "from sklearn.metrics import average_precision_score\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "import pandas as pd\n",
        "from itertools import zip_longest\n",
        "import re\n",
        "import tensorflow.compat.v2 as tf\n",
        "import tensorflow_hub as hub\n",
        "import bokeh\n",
        "import bokeh.models\n",
        "import bokeh.plotting\n",
        "import numpy as np\n",
        "import os\n",
        "import pandas as pd\n",
        "import tensorflow.compat.v2 as tf\n",
        "import tensorflow_hub as hub\n",
        "from tensorflow_text import SentencepieceTokenizer\n",
        "import sklearn.metrics.pairwise\n",
        "\n",
        "from simpleneighbors import SimpleNeighbors\n",
        "from tqdm import tqdm\n",
        "from tqdm import trange\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nLNFsO8L9C6A"
      },
      "source": [
        "# Download MODELS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BPnXzt3gGHr1",
        "outputId": "32098039-ddc5-4c62-8c96-b0e74eab8dfc"
      },
      "source": [
        "legal_ru_electra = \"1_FlnOa7SXmWtmLlmascK5EZHb3b3kHKF\"\n",
        "legal_ru_longformer = \"1bTIbrGIZ-vElBosEzaCP4Mc1ghNFu8NN\"\n",
        "legal_ru_bert = \"1Y8QqolrGGNqmaZKBM8OSk4CrV0QPPUAy\"\n",
        "\n",
        "models = ['legal_ru_electra', 'legal_ru_longformer', 'legal_ru_bert' ]\n",
        "id_model = [\"1_FlnOa7SXmWtmLlmascK5EZHb3b3kHKF\", \"1bTIbrGIZ-vElBosEzaCP4Mc1ghNFu8NN\", \"1Y8QqolrGGNqmaZKBM8OSk4CrV0QPPUAy\" ]\n",
        "#ID_MODEL = \"legal_ru_electra\" #@param [\"legal_ru_electra\", \"legal_ru_longformer\", \"legal_ru_bert\"]\n",
        "\n",
        "for id, model in zip(id_model, models):\n",
        "  file_name = f'{model}.zip'\n",
        "  print(file_name)\n",
        "  !wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=$id' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=$id\" -O file_name && rm -rf /tmp/cookies.txt\n",
        "  !unzip -uq file_name\n",
        "  module_url = f'/content/{model}'"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "legal_ru_electra.zip\n",
            "--2020-12-04 14:17:37--  https://docs.google.com/uc?export=download&confirm=dnfx&id=1_FlnOa7SXmWtmLlmascK5EZHb3b3kHKF\n",
            "Resolving docs.google.com (docs.google.com)... 172.217.15.110, 2607:f8b0:4004:805::200e\n",
            "Connecting to docs.google.com (docs.google.com)|172.217.15.110|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Moved Temporarily\n",
            "Location: https://doc-00-3c-docs.googleusercontent.com/docs/securesc/09nrapsmaplvt92emk9l2eobecsboine/omnd86sj3808of69n6nub422rrkidisg/1607091450000/18010547156067015791/08761343764223563897Z/1_FlnOa7SXmWtmLlmascK5EZHb3b3kHKF?e=download [following]\n",
            "--2020-12-04 14:17:37--  https://doc-00-3c-docs.googleusercontent.com/docs/securesc/09nrapsmaplvt92emk9l2eobecsboine/omnd86sj3808of69n6nub422rrkidisg/1607091450000/18010547156067015791/08761343764223563897Z/1_FlnOa7SXmWtmLlmascK5EZHb3b3kHKF?e=download\n",
            "Resolving doc-00-3c-docs.googleusercontent.com (doc-00-3c-docs.googleusercontent.com)... 172.217.9.193, 2607:f8b0:4004:806::2001\n",
            "Connecting to doc-00-3c-docs.googleusercontent.com (doc-00-3c-docs.googleusercontent.com)|172.217.9.193|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://docs.google.com/nonceSigner?nonce=1i6cjoa6cut3m&continue=https://doc-00-3c-docs.googleusercontent.com/docs/securesc/09nrapsmaplvt92emk9l2eobecsboine/omnd86sj3808of69n6nub422rrkidisg/1607091450000/18010547156067015791/08761343764223563897Z/1_FlnOa7SXmWtmLlmascK5EZHb3b3kHKF?e%3Ddownload&hash=rjtekdsn2rr8snrvd32h9ibt43b4louv [following]\n",
            "--2020-12-04 14:17:37--  https://docs.google.com/nonceSigner?nonce=1i6cjoa6cut3m&continue=https://doc-00-3c-docs.googleusercontent.com/docs/securesc/09nrapsmaplvt92emk9l2eobecsboine/omnd86sj3808of69n6nub422rrkidisg/1607091450000/18010547156067015791/08761343764223563897Z/1_FlnOa7SXmWtmLlmascK5EZHb3b3kHKF?e%3Ddownload&hash=rjtekdsn2rr8snrvd32h9ibt43b4louv\n",
            "Connecting to docs.google.com (docs.google.com)|172.217.15.110|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://doc-00-3c-docs.googleusercontent.com/docs/securesc/09nrapsmaplvt92emk9l2eobecsboine/omnd86sj3808of69n6nub422rrkidisg/1607091450000/18010547156067015791/08761343764223563897Z/1_FlnOa7SXmWtmLlmascK5EZHb3b3kHKF?e=download&nonce=1i6cjoa6cut3m&user=08761343764223563897Z&hash=ja28nucrnespq35fu4s1ivqutb3qa1uq [following]\n",
            "--2020-12-04 14:17:37--  https://doc-00-3c-docs.googleusercontent.com/docs/securesc/09nrapsmaplvt92emk9l2eobecsboine/omnd86sj3808of69n6nub422rrkidisg/1607091450000/18010547156067015791/08761343764223563897Z/1_FlnOa7SXmWtmLlmascK5EZHb3b3kHKF?e=download&nonce=1i6cjoa6cut3m&user=08761343764223563897Z&hash=ja28nucrnespq35fu4s1ivqutb3qa1uq\n",
            "Connecting to doc-00-3c-docs.googleusercontent.com (doc-00-3c-docs.googleusercontent.com)|172.217.9.193|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [application/zip]\n",
            "Saving to: ‘file_name’\n",
            "\n",
            "file_name               [     <=>            ]  43.26M  27.8MB/s    in 1.6s    \n",
            "\n",
            "2020-12-04 14:17:39 (27.8 MB/s) - ‘file_name’ saved [45362005]\n",
            "\n",
            "legal_ru_longformer.zip\n",
            "--2020-12-04 14:17:40--  https://docs.google.com/uc?export=download&confirm=4Zh2&id=1bTIbrGIZ-vElBosEzaCP4Mc1ghNFu8NN\n",
            "Resolving docs.google.com (docs.google.com)... 172.217.7.142, 2607:f8b0:4004:805::200e\n",
            "Connecting to docs.google.com (docs.google.com)|172.217.7.142|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Moved Temporarily\n",
            "Location: https://doc-00-60-docs.googleusercontent.com/docs/securesc/e6np2g3h12e365oqrij0uk4fqstrgela/3g697912mjiikguageb666cia6lef4ni/1607091450000/18010547156067015791/01950094563599007673Z/1bTIbrGIZ-vElBosEzaCP4Mc1ghNFu8NN?e=download [following]\n",
            "--2020-12-04 14:17:40--  https://doc-00-60-docs.googleusercontent.com/docs/securesc/e6np2g3h12e365oqrij0uk4fqstrgela/3g697912mjiikguageb666cia6lef4ni/1607091450000/18010547156067015791/01950094563599007673Z/1bTIbrGIZ-vElBosEzaCP4Mc1ghNFu8NN?e=download\n",
            "Resolving doc-00-60-docs.googleusercontent.com (doc-00-60-docs.googleusercontent.com)... 172.217.9.193, 2607:f8b0:4004:806::2001\n",
            "Connecting to doc-00-60-docs.googleusercontent.com (doc-00-60-docs.googleusercontent.com)|172.217.9.193|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://docs.google.com/nonceSigner?nonce=8i5qqfjv8voha&continue=https://doc-00-60-docs.googleusercontent.com/docs/securesc/e6np2g3h12e365oqrij0uk4fqstrgela/3g697912mjiikguageb666cia6lef4ni/1607091450000/18010547156067015791/01950094563599007673Z/1bTIbrGIZ-vElBosEzaCP4Mc1ghNFu8NN?e%3Ddownload&hash=o852jk4r6rc3atrcb7pk03ggnh6eajhh [following]\n",
            "--2020-12-04 14:17:40--  https://docs.google.com/nonceSigner?nonce=8i5qqfjv8voha&continue=https://doc-00-60-docs.googleusercontent.com/docs/securesc/e6np2g3h12e365oqrij0uk4fqstrgela/3g697912mjiikguageb666cia6lef4ni/1607091450000/18010547156067015791/01950094563599007673Z/1bTIbrGIZ-vElBosEzaCP4Mc1ghNFu8NN?e%3Ddownload&hash=o852jk4r6rc3atrcb7pk03ggnh6eajhh\n",
            "Connecting to docs.google.com (docs.google.com)|172.217.7.142|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://doc-00-60-docs.googleusercontent.com/docs/securesc/e6np2g3h12e365oqrij0uk4fqstrgela/3g697912mjiikguageb666cia6lef4ni/1607091450000/18010547156067015791/01950094563599007673Z/1bTIbrGIZ-vElBosEzaCP4Mc1ghNFu8NN?e=download&nonce=8i5qqfjv8voha&user=01950094563599007673Z&hash=u6jc1si94v6058ig9b4u7sujkjgc32uf [following]\n",
            "--2020-12-04 14:17:40--  https://doc-00-60-docs.googleusercontent.com/docs/securesc/e6np2g3h12e365oqrij0uk4fqstrgela/3g697912mjiikguageb666cia6lef4ni/1607091450000/18010547156067015791/01950094563599007673Z/1bTIbrGIZ-vElBosEzaCP4Mc1ghNFu8NN?e=download&nonce=8i5qqfjv8voha&user=01950094563599007673Z&hash=u6jc1si94v6058ig9b4u7sujkjgc32uf\n",
            "Connecting to doc-00-60-docs.googleusercontent.com (doc-00-60-docs.googleusercontent.com)|172.217.9.193|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [application/zip]\n",
            "Saving to: ‘file_name’\n",
            "\n",
            "file_name               [        <=>         ] 674.88M  78.2MB/s    in 14s     \n",
            "\n",
            "2020-12-04 14:17:55 (49.3 MB/s) - ‘file_name’ saved [707662788]\n",
            "\n",
            "legal_ru_bert.zip\n",
            "--2020-12-04 14:18:04--  https://docs.google.com/uc?export=download&confirm=LUnj&id=1Y8QqolrGGNqmaZKBM8OSk4CrV0QPPUAy\n",
            "Resolving docs.google.com (docs.google.com)... 172.217.7.142, 2607:f8b0:4004:805::200e\n",
            "Connecting to docs.google.com (docs.google.com)|172.217.7.142|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Moved Temporarily\n",
            "Location: https://doc-0s-10-docs.googleusercontent.com/docs/securesc/0gnfc66lpa6b3701dgvr1a5ohbdleqpt/a8p949q4ckuv6deuc0cc9vbh58arknr3/1607091450000/18010547156067015791/18353103388407433127Z/1Y8QqolrGGNqmaZKBM8OSk4CrV0QPPUAy?e=download [following]\n",
            "--2020-12-04 14:18:04--  https://doc-0s-10-docs.googleusercontent.com/docs/securesc/0gnfc66lpa6b3701dgvr1a5ohbdleqpt/a8p949q4ckuv6deuc0cc9vbh58arknr3/1607091450000/18010547156067015791/18353103388407433127Z/1Y8QqolrGGNqmaZKBM8OSk4CrV0QPPUAy?e=download\n",
            "Resolving doc-0s-10-docs.googleusercontent.com (doc-0s-10-docs.googleusercontent.com)... 172.217.9.193, 2607:f8b0:4004:806::2001\n",
            "Connecting to doc-0s-10-docs.googleusercontent.com (doc-0s-10-docs.googleusercontent.com)|172.217.9.193|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://docs.google.com/nonceSigner?nonce=f0sf0oeg5j954&continue=https://doc-0s-10-docs.googleusercontent.com/docs/securesc/0gnfc66lpa6b3701dgvr1a5ohbdleqpt/a8p949q4ckuv6deuc0cc9vbh58arknr3/1607091450000/18010547156067015791/18353103388407433127Z/1Y8QqolrGGNqmaZKBM8OSk4CrV0QPPUAy?e%3Ddownload&hash=uvd936omesthmo4mfj05oioidr1fpijk [following]\n",
            "--2020-12-04 14:18:04--  https://docs.google.com/nonceSigner?nonce=f0sf0oeg5j954&continue=https://doc-0s-10-docs.googleusercontent.com/docs/securesc/0gnfc66lpa6b3701dgvr1a5ohbdleqpt/a8p949q4ckuv6deuc0cc9vbh58arknr3/1607091450000/18010547156067015791/18353103388407433127Z/1Y8QqolrGGNqmaZKBM8OSk4CrV0QPPUAy?e%3Ddownload&hash=uvd936omesthmo4mfj05oioidr1fpijk\n",
            "Connecting to docs.google.com (docs.google.com)|172.217.7.142|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://doc-0s-10-docs.googleusercontent.com/docs/securesc/0gnfc66lpa6b3701dgvr1a5ohbdleqpt/a8p949q4ckuv6deuc0cc9vbh58arknr3/1607091450000/18010547156067015791/18353103388407433127Z/1Y8QqolrGGNqmaZKBM8OSk4CrV0QPPUAy?e=download&nonce=f0sf0oeg5j954&user=18353103388407433127Z&hash=jl5it7obsh6gujjcuc3g46gplcc8m4mg [following]\n",
            "--2020-12-04 14:18:04--  https://doc-0s-10-docs.googleusercontent.com/docs/securesc/0gnfc66lpa6b3701dgvr1a5ohbdleqpt/a8p949q4ckuv6deuc0cc9vbh58arknr3/1607091450000/18010547156067015791/18353103388407433127Z/1Y8QqolrGGNqmaZKBM8OSk4CrV0QPPUAy?e=download&nonce=f0sf0oeg5j954&user=18353103388407433127Z&hash=jl5it7obsh6gujjcuc3g46gplcc8m4mg\n",
            "Connecting to doc-0s-10-docs.googleusercontent.com (doc-0s-10-docs.googleusercontent.com)|172.217.9.193|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [application/zip]\n",
            "Saving to: ‘file_name’\n",
            "\n",
            "file_name               [            <=>     ] 359.48M  51.1MB/s    in 7.5s    \n",
            "\n",
            "2020-12-04 14:18:12 (47.8 MB/s) - ‘file_name’ saved [376946479]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LmUxFhAyFwKC"
      },
      "source": [
        "# DATASET"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4HtG6jGJFyQH",
        "outputId": "790ad493-6c3d-4380-8710-b9cc421a3e65"
      },
      "source": [
        "id = '1AwF05o4fKS9W9uam8QaTuncxwTLPOixK'\n",
        "\n",
        "!wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=$id' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=$id\" -O contract_labels-15000.zip && rm -rf /tmp/cookies.txt\n",
        "!unzip -uq contract_labels-15000.zip"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-12-04 14:49:19--  https://docs.google.com/uc?export=download&confirm=&id=1AwF05o4fKS9W9uam8QaTuncxwTLPOixK\n",
            "Resolving docs.google.com (docs.google.com)... 172.217.7.142, 2607:f8b0:4004:805::200e\n",
            "Connecting to docs.google.com (docs.google.com)|172.217.7.142|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Moved Temporarily\n",
            "Location: https://doc-14-bk-docs.googleusercontent.com/docs/securesc/kaff4aahktj38g05kl7mgd0pv02k28hr/6rb1j8m5vs20hb4ijt1jlcef0jkt4e2l/1607093325000/18010547156067015791/07164757107305388908Z/1AwF05o4fKS9W9uam8QaTuncxwTLPOixK?e=download [following]\n",
            "--2020-12-04 14:49:21--  https://doc-14-bk-docs.googleusercontent.com/docs/securesc/kaff4aahktj38g05kl7mgd0pv02k28hr/6rb1j8m5vs20hb4ijt1jlcef0jkt4e2l/1607093325000/18010547156067015791/07164757107305388908Z/1AwF05o4fKS9W9uam8QaTuncxwTLPOixK?e=download\n",
            "Resolving doc-14-bk-docs.googleusercontent.com (doc-14-bk-docs.googleusercontent.com)... 172.217.9.193, 2607:f8b0:4004:806::2001\n",
            "Connecting to doc-14-bk-docs.googleusercontent.com (doc-14-bk-docs.googleusercontent.com)|172.217.9.193|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://docs.google.com/nonceSigner?nonce=0tu7tnsispkme&continue=https://doc-14-bk-docs.googleusercontent.com/docs/securesc/kaff4aahktj38g05kl7mgd0pv02k28hr/6rb1j8m5vs20hb4ijt1jlcef0jkt4e2l/1607093325000/18010547156067015791/07164757107305388908Z/1AwF05o4fKS9W9uam8QaTuncxwTLPOixK?e%3Ddownload&hash=2dc5t7ok1511nq059kkfgtdjj76rm8lp [following]\n",
            "--2020-12-04 14:49:21--  https://docs.google.com/nonceSigner?nonce=0tu7tnsispkme&continue=https://doc-14-bk-docs.googleusercontent.com/docs/securesc/kaff4aahktj38g05kl7mgd0pv02k28hr/6rb1j8m5vs20hb4ijt1jlcef0jkt4e2l/1607093325000/18010547156067015791/07164757107305388908Z/1AwF05o4fKS9W9uam8QaTuncxwTLPOixK?e%3Ddownload&hash=2dc5t7ok1511nq059kkfgtdjj76rm8lp\n",
            "Connecting to docs.google.com (docs.google.com)|172.217.7.142|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://doc-14-bk-docs.googleusercontent.com/docs/securesc/kaff4aahktj38g05kl7mgd0pv02k28hr/6rb1j8m5vs20hb4ijt1jlcef0jkt4e2l/1607093325000/18010547156067015791/07164757107305388908Z/1AwF05o4fKS9W9uam8QaTuncxwTLPOixK?e=download&nonce=0tu7tnsispkme&user=07164757107305388908Z&hash=qif3dmdjeg6ljkqgprptkglvg0do06ud [following]\n",
            "--2020-12-04 14:49:21--  https://doc-14-bk-docs.googleusercontent.com/docs/securesc/kaff4aahktj38g05kl7mgd0pv02k28hr/6rb1j8m5vs20hb4ijt1jlcef0jkt4e2l/1607093325000/18010547156067015791/07164757107305388908Z/1AwF05o4fKS9W9uam8QaTuncxwTLPOixK?e=download&nonce=0tu7tnsispkme&user=07164757107305388908Z&hash=qif3dmdjeg6ljkqgprptkglvg0do06ud\n",
            "Connecting to doc-14-bk-docs.googleusercontent.com (doc-14-bk-docs.googleusercontent.com)|172.217.9.193|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [application/zip]\n",
            "Saving to: ‘contract_labels-15000.zip’\n",
            "\n",
            "contract_labels-150     [ <=>                ]   4.07M  --.-KB/s    in 0.07s   \n",
            "\n",
            "2020-12-04 14:49:21 (60.8 MB/s) - ‘contract_labels-15000.zip’ saved [4272884]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ve9K8GBXHITI"
      },
      "source": [
        "name_dataset = '/content/contract_labels-15000.tsv'"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7oFym4Hp83Jo"
      },
      "source": [
        "# MODELS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hhT77_30Ppc8"
      },
      "source": [
        "## GET EMBENDING"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 471
        },
        "id": "mLihcZjs8WuK",
        "outputId": "6ed475ca-adab-484b-9cc4-5f6942e97997"
      },
      "source": [
        "path = '/content/'   # путь к папке\n",
        "name_dataset = f'{path}contract_labels-15000.tsv'   # название полученного файла\n",
        "batch = 80                                           # количество строк проходимых через модель\n",
        "features_files = []\n",
        "\n",
        "\n",
        "module_urls = ['legal_ru_longformer', 'legal_ru_electra', 'legal_ru_bert']\n",
        "for module_url in module_urls:\n",
        "    if module_url == 'legal_ru_electra':\n",
        "      model = ElectraModel.from_pretrained(module_url)\n",
        "      tokenizer = ElectraTokenizerFast.from_pretrained(module_url)\n",
        "    elif module_url == 'legal_ru_longformer':\n",
        "      model = LongformerModel.from_pretrained(module_url)\n",
        "      tokenizer = LongformerModel.from_pretrained(module_url)\n",
        "    elif module_url == 'legal_ru_bert':\n",
        "      model = BertModel.from_pretrained(module_url)\n",
        "      tokenizer = BertTokenizer.from_pretrained(module_url)\n",
        "\n",
        "    features_file =  f'{path}{module_url}_features_batch_{batch}.txt' # файл с эмбендингами\n",
        "    device = torch.device(\"cuda:0\")\n",
        "    model.to(device)  \n",
        "\n",
        "    df = pd.read_csv(name_dataset, delimiter='\\t')\n",
        "    #print(df.head)\n",
        "    labels = df['labels']\n",
        "    text = df['text']\n",
        "    batch = batch\n",
        "    counter = 0\n",
        "    features_list = []\n",
        "    c=0\n",
        "    while counter < len(df):\n",
        "      print(\"Next Batch\")\n",
        "      t0 = time.time()\n",
        "      #try:\n",
        "      batch_1 = df[counter:counter+batch] \n",
        "      #print('batch_1', batch_1)\n",
        "      #print('!!!!!!!!', len(batch_1.values[0][0].split()))\n",
        "      #print(counter, counter+batch )\n",
        "      print(\"Got the batch!!!\")\n",
        "      counter = counter + batch\n",
        "      print(\"Counter is : \" + str(counter))\n",
        "      tokenized = batch_1['text'].apply(lambda x: tokenizer.encode(x, add_special_tokens=True))\n",
        "      print (\"tokenized!!\", tokenized)\n",
        "      max_len = 0\n",
        "      for i in tokenized.values:\n",
        "          if len(i) > max_len:\n",
        "              max_len = len(i)\n",
        "      padded = np.array([i + [0]*(max_len-len(i)) for i in tokenized.values])\n",
        "      print(\"padded!!\")\n",
        "      if module_url == 'legal_ru_longformer':\n",
        "        attention_mask = torch.ones(input_ids.shape, dtype=torch.long, device=input_ids.device).to(device) \n",
        "        attention_mask[:, [0,-1]] = 2 \n",
        "      elif module_url == 'legal_ru_electra' or module_url == 'legal_ru_electra':\n",
        "        attention_mask = np.where(padded != 0, 1, 0)\n",
        "        input_ids = torch.tensor(padded).to(device)\n",
        "        attention_mask = torch.tensor(attention_mask).to(device) \n",
        "      \n",
        "      print('attention_mask')\n",
        "      with torch.no_grad(): \n",
        "          last_hidden_states = model(input_ids, attention_mask=attention_mask)       \n",
        "          features = last_hidden_states[0][:,0,:].cpu().numpy()               \n",
        "          features_list.append(features)\n",
        "          with open(features_file, \"ab\" ) as f:          \n",
        "            np.savetxt(f, features, newline='\\n')\n",
        "            del features\n",
        "            torch.cuda.empty_cache()      \n",
        "          print('записали features')        \n",
        "            # import time\n",
        "            # time.sleep(60) # Сон в 60 секунд       \n",
        "      # except Exception as e:\n",
        "      #   print('Error!!!', e)\n",
        "      #   c+=1\n",
        "      print('len(features_list)', len(features_list))\n",
        "      print('len(labels)', len(labels))\n",
        "      print('len df text', len(text))\n",
        "      t1 = time.time()\n",
        "      total = t1-t0\n",
        "      print('Total time is: ', total)\n",
        "      print('количество выпадших строк = ', c*batch)\n",
        "      print('ГОТОВО!!!, файл с эмбендингами лежит в', features_file)\n",
        "      features_files.append(features_file)\n"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Next Batch\n",
            "Got the batch!!!\n",
            "Counter is : 80\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ModuleAttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleAttributeError\u001b[0m                      Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-b972b05a0eaf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     40\u001b[0m       \u001b[0mcounter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcounter\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m       \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Counter is : \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcounter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m       \u001b[0mtokenized\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m       \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"tokenized!!\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenized\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m       \u001b[0mmax_len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/series.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, convert_dtype, args, **kwds)\u001b[0m\n\u001b[1;32m   4210\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4211\u001b[0m                 \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4212\u001b[0;31m                 \u001b[0mmapped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_infer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4213\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4214\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSeries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/lib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m<ipython-input-21-b972b05a0eaf>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     40\u001b[0m       \u001b[0mcounter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcounter\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m       \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Counter is : \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcounter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m       \u001b[0mtokenized\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m       \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"tokenized!!\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenized\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m       \u001b[0mmax_len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    777\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    778\u001b[0m         raise ModuleAttributeError(\"'{}' object has no attribute '{}'\".format(\n\u001b[0;32m--> 779\u001b[0;31m             type(self).__name__, name))\n\u001b[0m\u001b[1;32m    780\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    781\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Module'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleAttributeError\u001b[0m: 'LongformerModel' object has no attribute 'encode'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9K7-5UoZSWGL"
      },
      "source": [
        "## classification"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WQiVpf2ORRO9"
      },
      "source": [
        "module_urls = ['legal_ru_electra', 'legal_ru_longformer', 'legal_ru_bert']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BNEG-66PS_e4"
      },
      "source": [
        "path = '/content/'   # путь к папке\n",
        "batch = 80   \n",
        "for module_url in module_urls:\n",
        "    df = pd.read_csv(name_dataset, delimiter='\\t')\n",
        "    features_file = f'{path}{module_url}_features_batch_{batch}.txt'\n",
        "    with open(features_file, \"rb\") as f:    \n",
        "        features = np.loadtxt(f)\n",
        "        len_features = len(features)\n",
        "        print(len(features))\n",
        "        labels = df['labels'][:len_features]\n",
        "        print(len(labels))\n",
        "        path = '/content/'                                   # путь к папке\n",
        "        name_dataset = f'{path}contract_labels-15000.tsv'    # название полученного файла\n",
        "        batch = 80                                           # количество строк проходимых через модель\n",
        "        labels = label_binarize(labels, classes=['качество', 'конфиденциальность','обязанности сторон','оплат', 'ответственность сторон', 'предмет договора', 'приёмка товара', 'прочие условия', 'споров', 'срок действия договора','сроки поставки', 'форс-мажор' ])\n",
        "        n_classes = labels.shape[1]\n",
        "        print('n_clases', n_classes )\n",
        "        train_features, test_features, train_labels, test_labels = train_test_split(features, labels, stratify=labels)\n",
        "\n",
        "\n",
        "      # Выбираем классификатор\n",
        "      names = [\n",
        "          'OVR_RandomForestClassifier'            \n",
        "              ]\n",
        "      # Задаем параметры классификатора\n",
        "      classifiers = [\n",
        "          OneVsRestClassifier(RandomForestClassifier())\n",
        "          ]\n",
        "\n",
        "      for name, clf in zip(names, classifiers):\n",
        "          t0 = time.time() \n",
        "          clf.fit(train_features, train_labels) \n",
        "          print (str(clf))\n",
        "          path_algoritm = f'{path}/{module_url}/{name_model}_{name}'\n",
        "          with open (path_algoritm, 'wb') as f:\n",
        "              pickle.dump(clf, f)    \n",
        "          # with open (path_algoritm, 'rb') as f:\n",
        "          #     clf =  pickle.load(f)\n",
        "          predictions = clf.predict(test_features)\n",
        "          print(metrics.classification_report(test_labels, predictions))\n",
        "          accuracy_score = metrics.accuracy_score(test_labels, predictions)\n",
        "          print('accuracy_score ', accuracy_score )   \n",
        "          #y_score = clf.decision_function(test_features)   \n",
        "          y_score = clf.predict_proba(test_features)\n",
        "\n",
        "          # Построение precision_recall_curve\n",
        "          precision = dict()\n",
        "          recall = dict()\n",
        "          average_precision = dict()\n",
        "          for i in range(n_classes):\n",
        "              precision[i], recall[i], _ = precision_recall_curve(test_labels[:, i],y_score[:, i])\n",
        "              average_precision[i] = average_precision_score(test_labels[:, i], y_score[:, i])\n",
        "\n",
        "          # A \"micro-average\": quantifying score on all classes jointly\n",
        "          precision[\"micro\"], recall[\"micro\"], _ = precision_recall_curve(test_labels.ravel(),y_score.ravel())\n",
        "          average_precision[\"micro\"] = average_precision_score(test_labels, y_score, average=\"micro\")\n",
        "          print('Average precision score, micro-averaged over all classes: {0:0.2f}'.format(average_precision[\"micro\"]))\n",
        "\n",
        "          plt.figure()\n",
        "          plt.step(recall['micro'], precision['micro'], where='post')\n",
        "          plt.xlabel('Recall')\n",
        "          plt.ylabel('Precision')\n",
        "          plt.ylim([0.0, 1.05])\n",
        "          plt.xlim([0.0, 1.0])\n",
        "          plt.title('{} Average precision score, micro-averaged over all classes: AP={}'.format(name, average_precision[\"micro\"]))\n",
        "          plt.show()\n",
        "          print('-=-=------------')    \n",
        "        \n",
        "          #Compute ROC curve and ROC area for each class\n",
        "          fpr = dict()\n",
        "          tpr = dict()\n",
        "          roc_auc = dict()\n",
        "          for i in range(n_classes):\n",
        "              fpr[i], tpr[i], _ = roc_curve(test_labels[:, i], y_score[:, i])\n",
        "              roc_auc[i] = auc(fpr[i], tpr[i])\n",
        "\n",
        "          # Compute micro-average ROC curve and ROC area\n",
        "          fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(test_labels.ravel(), y_score.ravel())\n",
        "          roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
        "\n",
        "          plt.figure()\n",
        "          lw = 2\n",
        "          plt.plot(fpr[2], tpr[2], color='darkorange',\n",
        "                  lw=lw, label='ROC curve (area = %0.2f)' % roc_auc[2])\n",
        "          plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
        "          plt.xlim([0.0, 1.0])\n",
        "          plt.ylim([0.0, 1.05])\n",
        "          plt.xlabel('False Positive Rate')\n",
        "          plt.ylabel('True Positive Rate')\n",
        "          plt.title(f'ROC curve: {name}')\n",
        "          plt.legend(loc=\"lower right\")\n",
        "          #plt.savefig(f'{name}.png')\n",
        "          plt.show()\n",
        "\n",
        "          t1 = time.time()\n",
        "          total = t1-t0\n",
        "        print('Total time is: ', total)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0itONqjvJ1Jy"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mkmF3w8WGLcM"
      },
      "source": [
        "model_type = LongformerModel #@param [\"ElectraModel\", \"AutoModel\", \"LongformerModel\", \"BertModel\"] {type:\"raw\", allow-input: true}\n",
        "model = model_type.from_pretrained(module_url) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TKPdAh58FJh0"
      },
      "source": [
        "tokenizer_type = BertTokenizer #@param [\"AutoTokenizer\", \"ElectraTokenizerFast\", \"LongformerTokenizer\", \"BertTokenizer\"] {type:\"raw\", allow-input: true}\n",
        "tokenizer = tokenizer_type.from_pretrained(module_url)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}