{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "LegalRuModels.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "IpxFIba8Gikk",
        "nLNFsO8L9C6A",
        "LmUxFhAyFwKC"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/devlab11/legal_ru_models/blob/main/LegalRuModels.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HE3ET2XW9GRH"
      },
      "source": [
        "**Copyright 2020 Mitlabs.**\n",
        "\n",
        "Licensed under the Apache License, Version 2.0 (the \"License\");"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1gHJ8GBc99GF"
      },
      "source": [
        "# LegalRuModels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gryfKbHd-Eu7"
      },
      "source": [
        "Фемида и трансформеры:\n",
        "использование лингвистических моделей на основе архитектуры “трансформер” в юридической практике. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MfBg1C5NB3X0"
      },
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  \n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/drive/1x2R3HjqFgYMDMp7uYMNm9tkBf9u4WkXR?authuser=1#scrollTo=MfBg1C5NB3X0\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://github.com/devlab11/legal_ru_models/blob/main/LegalRuModels.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View on GitHub</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a href=\"https://colab.research.google.com/drive/1x2R3HjqFgYMDMp7uYMNm9tkBf9u4WkXR?usp=sharing\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\" />Download notebook</a>\n",
        "  </td>\n",
        "  \n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IpxFIba8Gikk"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TSYZZrFOGh2q",
        "outputId": "89786086-c9d3-4377-f9a7-f77ac6f7638f"
      },
      "source": [
        "!pip install transformers\n",
        "!pip install tensorflow_text\n",
        "!pip install bokeh\n",
        "!pip install simpleneighbors[annoy]\n",
        "!pip install tqdm"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/88/b1/41130a228dd656a1a31ba281598a968320283f48d42782845f6ba567f00b/transformers-4.2.2-py3-none-any.whl (1.8MB)\n",
            "\u001b[K     |████████████████████████████████| 1.8MB 14.0MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 57.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.8)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from transformers) (3.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.8)\n",
            "Collecting tokenizers==0.9.4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0f/1c/e789a8b12e28be5bc1ce2156cf87cb522b379be9cadc7ad8091a4cc107c4/tokenizers-0.9.4-cp36-cp36m-manylinux2010_x86_64.whl (2.9MB)\n",
            "\u001b[K     |████████████████████████████████| 2.9MB 55.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.0.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.0)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893261 sha256=12f863fb7ce64628eea2c2d81cf1f2822d649e23ec31d54b81c788e3c3f7416e\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: sacremoses, tokenizers, transformers\n",
            "Successfully installed sacremoses-0.0.43 tokenizers-0.9.4 transformers-4.2.2\n",
            "Collecting tensorflow_text\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a0/86/22ad798f94d564c3e423758b60ddd3689e83ad629b3f31ff2ae45a6e3eed/tensorflow_text-2.4.3-cp36-cp36m-manylinux1_x86_64.whl (3.4MB)\n",
            "\u001b[K     |████████████████████████████████| 3.4MB 12.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: tensorflow<2.5,>=2.4.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow_text) (2.4.1)\n",
            "Requirement already satisfied: tensorflow-hub>=0.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow_text) (0.11.0)\n",
            "Requirement already satisfied: flatbuffers~=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.5,>=2.4.0->tensorflow_text) (1.12)\n",
            "Requirement already satisfied: astunparse~=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.5,>=2.4.0->tensorflow_text) (1.6.3)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.5,>=2.4.0->tensorflow_text) (3.12.4)\n",
            "Requirement already satisfied: tensorboard~=2.4 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.5,>=2.4.0->tensorflow_text) (2.4.1)\n",
            "Requirement already satisfied: grpcio~=1.32.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.5,>=2.4.0->tensorflow_text) (1.32.0)\n",
            "Requirement already satisfied: h5py~=2.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.5,>=2.4.0->tensorflow_text) (2.10.0)\n",
            "Requirement already satisfied: six~=1.15.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.5,>=2.4.0->tensorflow_text) (1.15.0)\n",
            "Requirement already satisfied: google-pasta~=0.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.5,>=2.4.0->tensorflow_text) (0.2.0)\n",
            "Requirement already satisfied: typing-extensions~=3.7.4 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.5,>=2.4.0->tensorflow_text) (3.7.4.3)\n",
            "Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.5,>=2.4.0->tensorflow_text) (0.3.3)\n",
            "Requirement already satisfied: termcolor~=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.5,>=2.4.0->tensorflow_text) (1.1.0)\n",
            "Requirement already satisfied: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.5,>=2.4.0->tensorflow_text) (1.1.2)\n",
            "Requirement already satisfied: wrapt~=1.12.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.5,>=2.4.0->tensorflow_text) (1.12.1)\n",
            "Requirement already satisfied: tensorflow-estimator<2.5.0,>=2.4.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.5,>=2.4.0->tensorflow_text) (2.4.0)\n",
            "Requirement already satisfied: wheel~=0.35 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.5,>=2.4.0->tensorflow_text) (0.36.2)\n",
            "Requirement already satisfied: numpy~=1.19.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.5,>=2.4.0->tensorflow_text) (1.19.5)\n",
            "Requirement already satisfied: absl-py~=0.10 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.5,>=2.4.0->tensorflow_text) (0.10.0)\n",
            "Requirement already satisfied: opt-einsum~=3.3.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.5,>=2.4.0->tensorflow_text) (3.3.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.9.2->tensorflow<2.5,>=2.4.0->tensorflow_text) (51.3.3)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard~=2.4->tensorflow<2.5,>=2.4.0->tensorflow_text) (1.17.2)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard~=2.4->tensorflow<2.5,>=2.4.0->tensorflow_text) (1.8.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard~=2.4->tensorflow<2.5,>=2.4.0->tensorflow_text) (3.3.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard~=2.4->tensorflow<2.5,>=2.4.0->tensorflow_text) (0.4.2)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard~=2.4->tensorflow<2.5,>=2.4.0->tensorflow_text) (2.23.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard~=2.4->tensorflow<2.5,>=2.4.0->tensorflow_text) (1.0.1)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow<2.5,>=2.4.0->tensorflow_text) (4.2.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow<2.5,>=2.4.0->tensorflow_text) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow<2.5,>=2.4.0->tensorflow_text) (4.7)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard~=2.4->tensorflow<2.5,>=2.4.0->tensorflow_text) (3.4.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow<2.5,>=2.4.0->tensorflow_text) (1.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow<2.5,>=2.4.0->tensorflow_text) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow<2.5,>=2.4.0->tensorflow_text) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow<2.5,>=2.4.0->tensorflow_text) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow<2.5,>=2.4.0->tensorflow_text) (1.24.3)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.6/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow<2.5,>=2.4.0->tensorflow_text) (0.4.8)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard~=2.4->tensorflow<2.5,>=2.4.0->tensorflow_text) (3.4.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow<2.5,>=2.4.0->tensorflow_text) (3.1.0)\n",
            "Installing collected packages: tensorflow-text\n",
            "Successfully installed tensorflow-text-2.4.3\n",
            "Requirement already satisfied: bokeh in /usr/local/lib/python3.6/dist-packages (2.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from bokeh) (2.8.1)\n",
            "Requirement already satisfied: PyYAML>=3.10 in /usr/local/lib/python3.6/dist-packages (from bokeh) (3.13)\n",
            "Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.6/dist-packages (from bokeh) (1.19.5)\n",
            "Requirement already satisfied: packaging>=16.8 in /usr/local/lib/python3.6/dist-packages (from bokeh) (20.8)\n",
            "Requirement already satisfied: pillow>=4.0 in /usr/local/lib/python3.6/dist-packages (from bokeh) (7.0.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.6/dist-packages (from bokeh) (3.7.4.3)\n",
            "Requirement already satisfied: Jinja2>=2.7 in /usr/local/lib/python3.6/dist-packages (from bokeh) (2.11.2)\n",
            "Requirement already satisfied: tornado>=5.1 in /usr/local/lib/python3.6/dist-packages (from bokeh) (5.1.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.1->bokeh) (1.15.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging>=16.8->bokeh) (2.4.7)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from Jinja2>=2.7->bokeh) (1.1.1)\n",
            "Collecting simpleneighbors[annoy]\n",
            "  Downloading https://files.pythonhosted.org/packages/f9/10/9092e15d9aa4a9e5a263416121f124e565766767e7866e11d7074ec50df5/simpleneighbors-0.1.0-py2.py3-none-any.whl\n",
            "Collecting annoy>=1.16.0; extra == \"annoy\"\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a1/5b/1c22129f608b3f438713b91cd880dc681d747a860afe3e8e0af86e921942/annoy-1.17.0.tar.gz (646kB)\n",
            "\u001b[K     |████████████████████████████████| 655kB 13.3MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: annoy\n",
            "  Building wheel for annoy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for annoy: filename=annoy-1.17.0-cp36-cp36m-linux_x86_64.whl size=393337 sha256=770ad47053cc1a4ee5216bb8b8493785f82634f7ac8633478e273f4124d23e42\n",
            "  Stored in directory: /root/.cache/pip/wheels/3a/c5/59/cce7e67b52c8e987389e53f917b6bb2a9d904a03246fadcb1e\n",
            "Successfully built annoy\n",
            "Installing collected packages: annoy, simpleneighbors\n",
            "Successfully installed annoy-1.17.0 simpleneighbors-0.1.0\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (4.41.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l63HYoQ9-FAu",
        "outputId": "121a8e36-6d4a-4255-8c1d-778641200fc1"
      },
      "source": [
        "!pip install laserembeddings[ru]\n",
        "from laserembeddings import Laser\n",
        "!python -m laserembeddings download-models"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting laserembeddings[ru]\n",
            "  Downloading https://files.pythonhosted.org/packages/a2/4b/a9e3ee9f4825bd2bb6b48f26370e2c341860ec0cb2a9a27deea9be6c2299/laserembeddings-1.1.0-py3-none-any.whl\n",
            "\u001b[33m  WARNING: laserembeddings 1.1.0 does not provide the extra 'ru'\u001b[0m\n",
            "Collecting subword-nmt<0.4.0,>=0.3.6\n",
            "  Downloading https://files.pythonhosted.org/packages/74/60/6600a7bc09e7ab38bc53a48a20d8cae49b837f93f5842a41fe513a694912/subword_nmt-0.3.7-py2.py3-none-any.whl\n",
            "Collecting transliterate==1.10.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a1/6e/9a9d597dbdd6d0172427c8cc07c35736471e631060df9e59eeb87687f817/transliterate-1.10.2-py2.py3-none-any.whl (45kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 8.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy<2.0.0,>=1.15.4 in /usr/local/lib/python3.6/dist-packages (from laserembeddings[ru]) (1.19.5)\n",
            "Requirement already satisfied: torch<2.0.0,>=1.0.1.post2 in /usr/local/lib/python3.6/dist-packages (from laserembeddings[ru]) (1.7.0+cu101)\n",
            "Collecting sacremoses==0.0.35\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1f/8e/ed5364a06a9ba720fddd9820155cc57300d28f5f43a6fd7b7e817177e642/sacremoses-0.0.35.tar.gz (859kB)\n",
            "\u001b[K     |████████████████████████████████| 860kB 31.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from transliterate==1.10.2->laserembeddings[ru]) (1.15.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch<2.0.0,>=1.0.1.post2->laserembeddings[ru]) (0.16.0)\n",
            "Requirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from torch<2.0.0,>=1.0.1.post2->laserembeddings[ru]) (0.8)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch<2.0.0,>=1.0.1.post2->laserembeddings[ru]) (3.7.4.3)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses==0.0.35->laserembeddings[ru]) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses==0.0.35->laserembeddings[ru]) (1.0.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from sacremoses==0.0.35->laserembeddings[ru]) (4.41.1)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.35-cp36-none-any.whl size=884000 sha256=2423028454829656eb40b811740171e74faca8fc38c23e6182155f59e4294964\n",
            "  Stored in directory: /root/.cache/pip/wheels/63/2a/db/63e2909042c634ef551d0d9ac825b2b0b32dede4a6d87ddc94\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: subword-nmt, transliterate, sacremoses, laserembeddings\n",
            "  Found existing installation: sacremoses 0.0.43\n",
            "    Uninstalling sacremoses-0.0.43:\n",
            "      Successfully uninstalled sacremoses-0.0.43\n",
            "Successfully installed laserembeddings-1.1.0 sacremoses-0.0.35 subword-nmt-0.3.7 transliterate-1.10.2\n",
            "Downloading models into /usr/local/lib/python3.6/dist-packages/laserembeddings/data\n",
            "\n",
            "✅   Downloaded https://dl.fbaipublicfiles.com/laser/models/93langs.fcodes    \n",
            "✅   Downloaded https://dl.fbaipublicfiles.com/laser/models/93langs.fvocab    \n",
            "✅   Downloaded https://dl.fbaipublicfiles.com/laser/models/bilstm.93langs.2018-12-26.pt    \n",
            "\n",
            "✨ You're all set!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T1ygvZ_3G4xO",
        "outputId": "7fe1bbbf-9b94-42f0-8d82-8c28acefe80a"
      },
      "source": [
        "import pickle\n",
        "import time\n",
        "import warnings\n",
        "\n",
        "import transformers as ppb\n",
        "warnings.filterwarnings('ignore')\n",
        "import bz2\n",
        "import csv\n",
        "import _pickle as cPickle\n",
        "from numpy import savetxt\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "from transformers import BertTokenizer, BertModel\n",
        "from transformers import AutoModel,AutoTokenizer, AutoModelWithLMHead\n",
        "from transformers import LongformerModel, LongformerTokenizer\n",
        "from transformers import (ElectraConfig, ElectraForMaskedLM,\n",
        "                          ElectraForPreTraining, ElectraForTokenClassification,\n",
        "                          ElectraModel, ElectraTokenizerFast)\n",
        "import pickle\n",
        "import _pickle as cPickle\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import gc\n",
        "from numpy import loadtxt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.preprocessing import normalize\n",
        "import matplotlib as plt\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import cross_validate\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "import time\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.gaussian_process import GaussianProcessClassifier\n",
        "from sklearn.gaussian_process.kernels import RBF\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import make_scorer, accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, auc, roc_curve\n",
        "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.feature_selection import SelectFromModel\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "from sklearn.preprocessing import label_binarize\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import precision_recall_curve\n",
        "from sklearn.metrics import plot_precision_recall_curve\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from sklearn.metrics import precision_recall_curve\n",
        "from sklearn.metrics import average_precision_score\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "import pandas as pd\n",
        "from itertools import zip_longest\n",
        "import re\n",
        "\n",
        "import bokeh\n",
        "import bokeh.models\n",
        "import bokeh.plotting\n",
        "import os\n",
        "import pandas as pd\n",
        "import tensorflow.compat.v2 as tf\n",
        "import tensorflow_hub as hub\n",
        "from tensorflow_text import SentencepieceTokenizer\n",
        "import sklearn.metrics.pairwise\n",
        "\n",
        "from simpleneighbors import SimpleNeighbors\n",
        "from tqdm import tqdm\n",
        "from tqdm import trange\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nLNFsO8L9C6A"
      },
      "source": [
        "# Download models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BPnXzt3gGHr1",
        "outputId": "e5b2fdac-5a4b-4299-b1fc-48009be28a2c"
      },
      "source": [
        "legal_ru_electra = \"1_FlnOa7SXmWtmLlmascK5EZHb3b3kHKF\"\n",
        "legal_ru_longformer = \"1bTIbrGIZ-vElBosEzaCP4Mc1ghNFu8NN\"\n",
        "legal_ru_bert = \"1Y8QqolrGGNqmaZKBM8OSk4CrV0QPPUAy\"\n",
        "\n",
        "models = ['legal_ru_electra', 'legal_ru_longformer', 'legal_ru_bert' ]\n",
        "id_model = [\"1_FlnOa7SXmWtmLlmascK5EZHb3b3kHKF\", \"1bTIbrGIZ-vElBosEzaCP4Mc1ghNFu8NN\", \"1Y8QqolrGGNqmaZKBM8OSk4CrV0QPPUAy\" ]\n",
        "#ID_MODEL = \"legal_ru_electra\" #@param [\"legal_ru_electra\", \"legal_ru_longformer\", \"legal_ru_bert\"]\n",
        "\n",
        "for id, model in zip(id_model, models):\n",
        "  file_name = f'{model}.zip'\n",
        "  print(file_name)\n",
        "  !wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=$id' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=$id\" -O file_name && rm -rf /tmp/cookies.txt\n",
        "  !unzip -uq file_name\n",
        "  module_url = f'/content/{model}'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "legal_ru_electra.zip\n",
            "--2021-01-29 08:59:19--  https://docs.google.com/uc?export=download&confirm=aXBw&id=1_FlnOa7SXmWtmLlmascK5EZHb3b3kHKF\n",
            "Resolving docs.google.com (docs.google.com)... 108.177.126.113, 108.177.126.100, 108.177.126.101, ...\n",
            "Connecting to docs.google.com (docs.google.com)|108.177.126.113|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Moved Temporarily\n",
            "Location: https://doc-0o-5g-docs.googleusercontent.com/docs/securesc/eqolldomiaq3f5r452lahfd68r1f6a42/94rta58q28545fpsikjeiou7601e0pct/1611910725000/18010547156067015791/15357915185416053304Z/1_FlnOa7SXmWtmLlmascK5EZHb3b3kHKF?e=download [following]\n",
            "--2021-01-29 08:59:19--  https://doc-0o-5g-docs.googleusercontent.com/docs/securesc/eqolldomiaq3f5r452lahfd68r1f6a42/94rta58q28545fpsikjeiou7601e0pct/1611910725000/18010547156067015791/15357915185416053304Z/1_FlnOa7SXmWtmLlmascK5EZHb3b3kHKF?e=download\n",
            "Resolving doc-0o-5g-docs.googleusercontent.com (doc-0o-5g-docs.googleusercontent.com)... 108.177.119.132, 2a00:1450:4013:c00::84\n",
            "Connecting to doc-0o-5g-docs.googleusercontent.com (doc-0o-5g-docs.googleusercontent.com)|108.177.119.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://docs.google.com/nonceSigner?nonce=t7cdqaj0r1uh0&continue=https://doc-0o-5g-docs.googleusercontent.com/docs/securesc/eqolldomiaq3f5r452lahfd68r1f6a42/94rta58q28545fpsikjeiou7601e0pct/1611910725000/18010547156067015791/15357915185416053304Z/1_FlnOa7SXmWtmLlmascK5EZHb3b3kHKF?e%3Ddownload&hash=n95qkh6hhje7s89eaflk1o0iicv8rmbf [following]\n",
            "--2021-01-29 08:59:19--  https://docs.google.com/nonceSigner?nonce=t7cdqaj0r1uh0&continue=https://doc-0o-5g-docs.googleusercontent.com/docs/securesc/eqolldomiaq3f5r452lahfd68r1f6a42/94rta58q28545fpsikjeiou7601e0pct/1611910725000/18010547156067015791/15357915185416053304Z/1_FlnOa7SXmWtmLlmascK5EZHb3b3kHKF?e%3Ddownload&hash=n95qkh6hhje7s89eaflk1o0iicv8rmbf\n",
            "Connecting to docs.google.com (docs.google.com)|108.177.126.113|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://doc-0o-5g-docs.googleusercontent.com/docs/securesc/eqolldomiaq3f5r452lahfd68r1f6a42/94rta58q28545fpsikjeiou7601e0pct/1611910725000/18010547156067015791/15357915185416053304Z/1_FlnOa7SXmWtmLlmascK5EZHb3b3kHKF?e=download&nonce=t7cdqaj0r1uh0&user=15357915185416053304Z&hash=hjbnrqqjl5qogbti168oi0g0a2gjdujv [following]\n",
            "--2021-01-29 08:59:20--  https://doc-0o-5g-docs.googleusercontent.com/docs/securesc/eqolldomiaq3f5r452lahfd68r1f6a42/94rta58q28545fpsikjeiou7601e0pct/1611910725000/18010547156067015791/15357915185416053304Z/1_FlnOa7SXmWtmLlmascK5EZHb3b3kHKF?e=download&nonce=t7cdqaj0r1uh0&user=15357915185416053304Z&hash=hjbnrqqjl5qogbti168oi0g0a2gjdujv\n",
            "Connecting to doc-0o-5g-docs.googleusercontent.com (doc-0o-5g-docs.googleusercontent.com)|108.177.119.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [application/zip]\n",
            "Saving to: ‘file_name’\n",
            "\n",
            "file_name               [  <=>               ]  43.26M   181MB/s    in 0.2s    \n",
            "\n",
            "2021-01-29 08:59:20 (181 MB/s) - ‘file_name’ saved [45362005]\n",
            "\n",
            "legal_ru_longformer.zip\n",
            "--2021-01-29 08:59:21--  https://docs.google.com/uc?export=download&confirm=6i0m&id=1bTIbrGIZ-vElBosEzaCP4Mc1ghNFu8NN\n",
            "Resolving docs.google.com (docs.google.com)... 108.177.126.113, 108.177.126.102, 108.177.126.138, ...\n",
            "Connecting to docs.google.com (docs.google.com)|108.177.126.113|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Moved Temporarily\n",
            "Location: https://doc-14-5g-docs.googleusercontent.com/docs/securesc/ifk908g3cqvonhbr8sbvo63l8d9gc2lg/nedpkhbkrtop7lrn2vo3v76u1lrgon6q/1611910725000/18010547156067015791/02360565461017258888Z/1bTIbrGIZ-vElBosEzaCP4Mc1ghNFu8NN?e=download [following]\n",
            "--2021-01-29 08:59:21--  https://doc-14-5g-docs.googleusercontent.com/docs/securesc/ifk908g3cqvonhbr8sbvo63l8d9gc2lg/nedpkhbkrtop7lrn2vo3v76u1lrgon6q/1611910725000/18010547156067015791/02360565461017258888Z/1bTIbrGIZ-vElBosEzaCP4Mc1ghNFu8NN?e=download\n",
            "Resolving doc-14-5g-docs.googleusercontent.com (doc-14-5g-docs.googleusercontent.com)... 108.177.119.132, 2a00:1450:4013:c00::84\n",
            "Connecting to doc-14-5g-docs.googleusercontent.com (doc-14-5g-docs.googleusercontent.com)|108.177.119.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://docs.google.com/nonceSigner?nonce=r9jdiosp2ifei&continue=https://doc-14-5g-docs.googleusercontent.com/docs/securesc/ifk908g3cqvonhbr8sbvo63l8d9gc2lg/nedpkhbkrtop7lrn2vo3v76u1lrgon6q/1611910725000/18010547156067015791/02360565461017258888Z/1bTIbrGIZ-vElBosEzaCP4Mc1ghNFu8NN?e%3Ddownload&hash=c1obc95skifps67g6esrk5hpi3vk0d9d [following]\n",
            "--2021-01-29 08:59:22--  https://docs.google.com/nonceSigner?nonce=r9jdiosp2ifei&continue=https://doc-14-5g-docs.googleusercontent.com/docs/securesc/ifk908g3cqvonhbr8sbvo63l8d9gc2lg/nedpkhbkrtop7lrn2vo3v76u1lrgon6q/1611910725000/18010547156067015791/02360565461017258888Z/1bTIbrGIZ-vElBosEzaCP4Mc1ghNFu8NN?e%3Ddownload&hash=c1obc95skifps67g6esrk5hpi3vk0d9d\n",
            "Connecting to docs.google.com (docs.google.com)|108.177.126.113|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://doc-14-5g-docs.googleusercontent.com/docs/securesc/ifk908g3cqvonhbr8sbvo63l8d9gc2lg/nedpkhbkrtop7lrn2vo3v76u1lrgon6q/1611910725000/18010547156067015791/02360565461017258888Z/1bTIbrGIZ-vElBosEzaCP4Mc1ghNFu8NN?e=download&nonce=r9jdiosp2ifei&user=02360565461017258888Z&hash=f1qs2ls9j94bfket8qr0qcffucmvee1u [following]\n",
            "--2021-01-29 08:59:22--  https://doc-14-5g-docs.googleusercontent.com/docs/securesc/ifk908g3cqvonhbr8sbvo63l8d9gc2lg/nedpkhbkrtop7lrn2vo3v76u1lrgon6q/1611910725000/18010547156067015791/02360565461017258888Z/1bTIbrGIZ-vElBosEzaCP4Mc1ghNFu8NN?e=download&nonce=r9jdiosp2ifei&user=02360565461017258888Z&hash=f1qs2ls9j94bfket8qr0qcffucmvee1u\n",
            "Connecting to doc-14-5g-docs.googleusercontent.com (doc-14-5g-docs.googleusercontent.com)|108.177.119.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [application/zip]\n",
            "Saving to: ‘file_name’\n",
            "\n",
            "file_name               [              <=>   ] 674.88M   162MB/s    in 4.5s    \n",
            "\n",
            "2021-01-29 08:59:27 (151 MB/s) - ‘file_name’ saved [707662788]\n",
            "\n",
            "legal_ru_bert.zip\n",
            "--2021-01-29 08:59:34--  https://docs.google.com/uc?export=download&confirm=jKh&id=1Y8QqolrGGNqmaZKBM8OSk4CrV0QPPUAy\n",
            "Resolving docs.google.com (docs.google.com)... 108.177.126.138, 108.177.126.101, 108.177.126.139, ...\n",
            "Connecting to docs.google.com (docs.google.com)|108.177.126.138|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: ‘file_name’\n",
            "\n",
            "file_name               [ <=>                ]   3.19K  --.-KB/s    in 0s      \n",
            "\n",
            "2021-01-29 08:59:35 (40.7 MB/s) - ‘file_name’ saved [3262]\n",
            "\n",
            "[file_name]\n",
            "  End-of-central-directory signature not found.  Either this file is not\n",
            "  a zipfile, or it constitutes one disk of a multi-part archive.  In the\n",
            "  latter case the central directory and zipfile comment will be found on\n",
            "  the last disk(s) of this archive.\n",
            "unzip:  cannot find zipfile directory in one of file_name or\n",
            "        file_name.zip, and cannot find file_name.ZIP, period.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LmUxFhAyFwKC"
      },
      "source": [
        "# Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4HtG6jGJFyQH",
        "outputId": "4227cf84-3bde-4760-fe13-b2d22880230a"
      },
      "source": [
        "id = '1AwF05o4fKS9W9uam8QaTuncxwTLPOixK'\n",
        "\n",
        "!wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=$id' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=$id\" -O contract_labels-15000.zip && rm -rf /tmp/cookies.txt\n",
        "!unzip -uq contract_labels-15000.zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-01-29 08:59:37--  https://docs.google.com/uc?export=download&confirm=&id=1AwF05o4fKS9W9uam8QaTuncxwTLPOixK\n",
            "Resolving docs.google.com (docs.google.com)... 108.177.126.113, 108.177.126.102, 108.177.126.138, ...\n",
            "Connecting to docs.google.com (docs.google.com)|108.177.126.113|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Moved Temporarily\n",
            "Location: https://doc-0g-2s-docs.googleusercontent.com/docs/securesc/9k72vg968ng7jjq84f7v5s2903gd4pt7/5hrs89vtft6dl9mb6e5v5su95edqghq8/1611910725000/18010547156067015791/03880698882477707887Z/1AwF05o4fKS9W9uam8QaTuncxwTLPOixK?e=download [following]\n",
            "--2021-01-29 08:59:38--  https://doc-0g-2s-docs.googleusercontent.com/docs/securesc/9k72vg968ng7jjq84f7v5s2903gd4pt7/5hrs89vtft6dl9mb6e5v5su95edqghq8/1611910725000/18010547156067015791/03880698882477707887Z/1AwF05o4fKS9W9uam8QaTuncxwTLPOixK?e=download\n",
            "Resolving doc-0g-2s-docs.googleusercontent.com (doc-0g-2s-docs.googleusercontent.com)... 108.177.119.132, 2a00:1450:4013:c00::84\n",
            "Connecting to doc-0g-2s-docs.googleusercontent.com (doc-0g-2s-docs.googleusercontent.com)|108.177.119.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://docs.google.com/nonceSigner?nonce=edbbqd9pjr3d0&continue=https://doc-0g-2s-docs.googleusercontent.com/docs/securesc/9k72vg968ng7jjq84f7v5s2903gd4pt7/5hrs89vtft6dl9mb6e5v5su95edqghq8/1611910725000/18010547156067015791/03880698882477707887Z/1AwF05o4fKS9W9uam8QaTuncxwTLPOixK?e%3Ddownload&hash=o5n5k2837qt8tlvvd1pd4frpqcq3g202 [following]\n",
            "--2021-01-29 08:59:38--  https://docs.google.com/nonceSigner?nonce=edbbqd9pjr3d0&continue=https://doc-0g-2s-docs.googleusercontent.com/docs/securesc/9k72vg968ng7jjq84f7v5s2903gd4pt7/5hrs89vtft6dl9mb6e5v5su95edqghq8/1611910725000/18010547156067015791/03880698882477707887Z/1AwF05o4fKS9W9uam8QaTuncxwTLPOixK?e%3Ddownload&hash=o5n5k2837qt8tlvvd1pd4frpqcq3g202\n",
            "Connecting to docs.google.com (docs.google.com)|108.177.126.113|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://doc-0g-2s-docs.googleusercontent.com/docs/securesc/9k72vg968ng7jjq84f7v5s2903gd4pt7/5hrs89vtft6dl9mb6e5v5su95edqghq8/1611910725000/18010547156067015791/03880698882477707887Z/1AwF05o4fKS9W9uam8QaTuncxwTLPOixK?e=download&nonce=edbbqd9pjr3d0&user=03880698882477707887Z&hash=mc5q01n9vv756u19l30hvig3bc0rddlr [following]\n",
            "--2021-01-29 08:59:39--  https://doc-0g-2s-docs.googleusercontent.com/docs/securesc/9k72vg968ng7jjq84f7v5s2903gd4pt7/5hrs89vtft6dl9mb6e5v5su95edqghq8/1611910725000/18010547156067015791/03880698882477707887Z/1AwF05o4fKS9W9uam8QaTuncxwTLPOixK?e=download&nonce=edbbqd9pjr3d0&user=03880698882477707887Z&hash=mc5q01n9vv756u19l30hvig3bc0rddlr\n",
            "Connecting to doc-0g-2s-docs.googleusercontent.com (doc-0g-2s-docs.googleusercontent.com)|108.177.119.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [application/zip]\n",
            "Saving to: ‘contract_labels-15000.zip’\n",
            "\n",
            "contract_labels-150     [ <=>                ]   4.07M  --.-KB/s    in 0.04s   \n",
            "\n",
            "2021-01-29 08:59:39 (106 MB/s) - ‘contract_labels-15000.zip’ saved [4272884]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ve9K8GBXHITI"
      },
      "source": [
        "name_dataset = '/content/contract_labels-15000.tsv'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hhT77_30Ppc8"
      },
      "source": [
        "# Creating Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6vwk4RWfaAcs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "71f42905-242d-4033-bd18-1ff5380ea690"
      },
      "source": [
        "url = 'https://tfhub.dev/google/universal-sentence-encoder-multilingual/3'\n",
        "embed_text = hub.load(url)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:absl:Using /tmp/tfhub_modules to cache modules.\n",
            "INFO:absl:Downloading TF-Hub Module 'https://tfhub.dev/google/universal-sentence-encoder-multilingual/3'.\n",
            "INFO:absl:Downloaded https://tfhub.dev/google/universal-sentence-encoder-multilingual/3, Total size: 266.88MB\n",
            "INFO:absl:Downloaded TF-Hub Module 'https://tfhub.dev/google/universal-sentence-encoder-multilingual/3'.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 453
        },
        "id": "mLihcZjs8WuK",
        "outputId": "060b9a69-3865-42c9-eae6-ca38ac31eb66"
      },
      "source": [
        "#path = '/content/'   # путь к папке\n",
        "path = '/content/drive/MyDrive/MODELS/create_embed/'\n",
        "name_dataset = f'{path}contract_labels-3000.tsv'   # название полученного файла\n",
        "batch = 50                                           # количество строк проходимых через модель\n",
        "features_files = []\n",
        "module_urls = [ 'legal_ru_longformer', 'legal_ru_electra', 'legal_ru_bert' ,'laser','rubert', 'muse' ]\n",
        "for module_url in module_urls:\n",
        "    if module_url == 'muse':\n",
        "      pass      \n",
        "    elif module_url == 'laser':\n",
        "       model = Laser()  \n",
        "    else:   \n",
        "      if module_url == 'legal_ru_electra':\n",
        "        model = ElectraModel.from_pretrained(module_url)\n",
        "        tokenizer = ElectraTokenizerFast.from_pretrained(module_url)        \n",
        "      elif module_url == 'legal_ru_longformer':\n",
        "        model = LongformerModel.from_pretrained(module_url)\n",
        "        tokenizer = LongformerTokenizer.from_pretrained(module_url)        \n",
        "      elif module_url == 'legal_ru_bert':\n",
        "        model = BertModel.from_pretrained(module_url)\n",
        "        tokenizer = BertTokenizer.from_pretrained(module_url) \n",
        "      elif module_url == 'rubert':\n",
        "        tokenizer = AutoTokenizer.from_pretrained(\"DeepPavlov/rubert-base-cased\")\n",
        "        model = AutoModel.from_pretrained(\"DeepPavlov/rubert-base-cased\")         \n",
        "      device = torch.device(\"cuda:0\")\n",
        "      model.to(device) \n",
        "    features_file =  f'{path}{module_url}_features_batch_{batch}.txt' # файл с эмбендингами    \n",
        "    df = pd.read_csv(name_dataset, delimiter='\\t')\n",
        "    print(df, len(df))\n",
        "    labels = df['labels']\n",
        "    text = df['text']\n",
        "    counter = 0\n",
        "    features_list = []\n",
        "    c=0\n",
        "    while counter < len(df):\n",
        "      t0 = time.time()\n",
        "      try:\n",
        "        batch_1 = df[counter:counter+batch]        \n",
        "        counter = counter + batch\n",
        "        print(\"Counter is : \" + str(counter))\n",
        "        if module_url == 'muse':                          \n",
        "            features = embed_text(batch_1)\n",
        "            features_list.append(features)\n",
        "            with open(features_file, \"ab\" ) as f:          \n",
        "                  np.savetxt(f, features, newline='\\n')\n",
        "                  del features\n",
        "                  torch.cuda.empty_cache()      \n",
        "        elif module_url == 'laser':\n",
        "            features = model.embed_sentences(batch_1,  lang='ru')            \n",
        "            features_list.append(features)\n",
        "            with open(features_file, \"ab\" ) as f:          \n",
        "                  np.savetxt(f, features, newline='\\n')\n",
        "                  del features\n",
        "                  torch.cuda.empty_cache()      \n",
        "        else:        \n",
        "            if module_url == 'legal_ru_longformer':\n",
        "              tokenized = batch_1['text'].apply(lambda x: tokenizer.encode(x, add_special_tokens=True))              \n",
        "              max_len = 0\n",
        "              for i in tokenized.values:\n",
        "                  if len(i) > max_len:\n",
        "                      max_len = len(i)\n",
        "              padded = np.array([i + [0]*(max_len-len(i)) for i in tokenized.values])              \n",
        "              input_ids = torch.tensor(padded).to(device)\n",
        "              attention_mask = torch.ones(input_ids.shape, dtype=torch.long, device=input_ids.device).to(device) \n",
        "              attention_mask[:, [0,-1]] = 2 \n",
        "            elif module_url == 'legal_ru_electra' or module_url == 'legal_ru_bert' or module_url == 'rubert':\n",
        "              tokenized = batch_1['text'].apply(lambda x: tokenizer.encode(x, add_special_tokens=True))              \n",
        "              max_len = 0\n",
        "              for i in tokenized.values:\n",
        "                  if len(i) > max_len:\n",
        "                      max_len = len(i)\n",
        "              padded = np.array([i + [0]*(max_len-len(i)) for i in tokenized.values])              \n",
        "              attention_mask = np.where(padded != 0, 1, 0)\n",
        "              input_ids = torch.tensor(padded).to(device)\n",
        "              attention_mask = torch.tensor(attention_mask).to(device)              \n",
        "            with torch.no_grad(): \n",
        "                last_hidden_states = model(input_ids, attention_mask=attention_mask)       \n",
        "                features = last_hidden_states[0][:,0,:].cpu().numpy()               \n",
        "                features_list.append(features)\n",
        "                with open(features_file, \"ab\" ) as f:          \n",
        "                  np.savetxt(f, features, newline='\\n')\n",
        "                  del features\n",
        "                  torch.cuda.empty_cache()      \n",
        "      except Exception as e:\n",
        "        print('Error!!!', e)\n",
        "        c+=1      \n",
        "      t1 = time.time()\n",
        "      total = t1-t0\n",
        "      print('Total time is: ', total)\n",
        "      print('количество выпадших строк = ', c*batch)\n",
        "      print('ГОТОВО!!!, файл с эмбендингами лежит в', features_file)\n",
        "      features_files.append(features_file)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-d4c71f9946f3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m       \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0mfeatures_file\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0;34mf'{path}{module_url}_features_batch_{batch}.txt'\u001b[0m \u001b[0;31m# файл с эмбендингами\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'\\t'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'labels'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    686\u001b[0m     )\n\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 688\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    689\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 454\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    946\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    947\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 948\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    950\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1178\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1180\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1181\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   2008\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2009\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2010\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2011\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2012\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/MODELS/create_embed/contract_labels-3000.tsv'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9K7-5UoZSWGL"
      },
      "source": [
        "# Classification"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WQiVpf2ORRO9"
      },
      "source": [
        "module_urls = ['muse','tfidf','laser',  'legal_ru_longformer', 'legal_ru_electra', 'legal_ru_bert','rubert']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UA3-rkPMltR6"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BNEG-66PS_e4"
      },
      "source": [
        "#path = '/content/'   # путь к папке\n",
        "path = '/content/drive/MyDrive/MODELS/create_embed/'\n",
        "batch = 50   \n",
        "metrics_dict = {}\n",
        "name_dataset = f'{path}contract_labels-15000.tsv'\n",
        "metrics_dict['Metric (weighted)'] = ['precision', 'recall', 'f1']\n",
        "for module_url in module_urls:\n",
        "      print('module_url: ', module_url)\n",
        "      df = pd.read_csv(name_dataset, delimiter='\\t')\n",
        "      if module_url == 'tfidf':\n",
        "        print('tfidf')\n",
        "        data =  df['text']\n",
        "        tf_idf_vectorizor =  TfidfVectorizer(max_features = 20)\n",
        "        tf_idf = tf_idf_vectorizor.fit_transform( data)\n",
        "        tf_idf_norm =  normalize(tf_idf)\n",
        "        features = tf_idf_norm.toarray()\n",
        "      else:\n",
        "        features_file = f'{path}{module_url}_features_batch_{batch}.txt'\n",
        "        with open(features_file, \"rb\") as f:    \n",
        "            features = np.loadtxt(f)\n",
        "      len_features = len(features)\n",
        "      print(len(features))\n",
        "      labels = df['labels'][:len_features]\n",
        "      print(len(labels))                                            \n",
        "      labels = label_binarize(labels, classes=['качество', 'конфиденциальность','обязанности сторон','оплат', 'ответственность сторон', 'предмет договора', 'приёмка товара', 'прочие условия', 'споров', 'срок действия договора','сроки поставки', 'форс-мажор' ])\n",
        "      n_classes = labels.shape[1]\n",
        "      print(n_classes )\n",
        "      train_features, test_features, train_labels, test_labels = train_test_split(features, labels, test_size = 0.9)\n",
        "      # Выбираем классификатор\n",
        "      names = ['OVR_RandomForestClassifier']\n",
        "      # Задаем параметры классификатора\n",
        "      classifiers = [\n",
        "          OneVsRestClassifier(RandomForestClassifier())\n",
        "          ]\n",
        "      for name, clf in zip(names, classifiers):\n",
        "          t0 = time.time() \n",
        "          #clf.fit(train_features, train_labels) \n",
        "          print (str(clf))\n",
        "          path_algoritm = f'{path}/{module_url}_{name}'\n",
        "          # with open (path_algoritm, 'wb') as f:\n",
        "          #     pickle.dump(clf, f)    \n",
        "          with open (path_algoritm, 'rb') as f:\n",
        "              clf =  pickle.load(f)\n",
        "          predictions = clf.predict(test_features)\n",
        "          print(metrics.classification_report(test_labels, predictions))\n",
        "          accuracy_score = metrics.accuracy_score(test_labels, predictions)\n",
        "          print('accuracy_score ', accuracy_score )   \n",
        "          #y_score = clf.decision_function(test_features)   \n",
        "          y_score = clf.predict_proba(test_features)          \n",
        "          #precision_recall_curve\n",
        "          precision = dict()\n",
        "          recall = dict()\n",
        "          average_precision = dict()\n",
        "          for i in range(n_classes):\n",
        "              precision[i], recall[i], _ = precision_recall_curve(test_labels[:, i],y_score[:, i])\n",
        "              average_precision[i] = average_precision_score(test_labels[:, i], y_score[:, i])\n",
        "          # A \"micro-average\": quantifying score on all classes jointly\n",
        "          precision[\"micro\"], recall[\"micro\"], _ = precision_recall_curve(test_labels.ravel(),y_score.ravel())\n",
        "          average_precision[\"micro\"] = average_precision_score(test_labels, y_score, average=\"micro\")\n",
        "          print('Average precision score, micro-averaged over all classes: {0:0.2f}'.format(average_precision[\"micro\"]))\n",
        "          # Заполнение таблицы со значениеями          \n",
        "          precision_score = metrics.precision_score(test_labels, predictions, average='weighted')\n",
        "          recall_score = metrics.recall_score(test_labels, predictions, average='weighted')\n",
        "          f1_score = metrics.f1_score(test_labels, predictions, average='weighted')          \n",
        "          print(f'{module_url}: recall_score={recall_score},  precision_score={precision_score},  f1_score={f1_score} ')          \n",
        "          roc_auc = dict()          \n",
        "          fpr = dict()\n",
        "          tpr = dict()\n",
        "          roc_auc = dict()\n",
        "          for i in range(n_classes):\n",
        "              fpr[i], tpr[i], _ = roc_curve(test_labels[:, i], y_score[:, i])\n",
        "              roc_auc[i] = auc(fpr[i], tpr[i])\n",
        "              # Compute micro-average ROC curve and ROC area\n",
        "              fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(test_labels.ravel(), y_score.ravel())\n",
        "              roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])             \n",
        "          metrics_dict['Metric (weighted)'] = ['PRECISION', 'RECALL', 'f1', 'ROC-AUC', 'AVERAGE PRECISION SCORE']    \n",
        "          metrics_dict[module_url] = [precision_score, recall_score, f1_score, roc_auc[\"micro\"] , average_precision[\"micro\"]]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AdsnVFl_6mCQ"
      },
      "source": [
        "df1 = pd.DataFrame((metrics_dict), columns = list(metrics_dict.keys()), index = None)\n",
        "df1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aUO4m5N3Wo35"
      },
      "source": [
        "## precision_recall_curve"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jUQYU9z_9Xd0"
      },
      "source": [
        "module_urls = [ 'tfidf',  'legal_ru_longformer', 'legal_ru_electra', 'legal_ru_bert', 'rubert']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hEKeNG1IVLhr"
      },
      "source": [
        "#path = '/content/'   # путь к папке\n",
        "batch = 50\n",
        "name_dataset = f'{path}contract_labels-3000.tsv'\n",
        "fig, axs = plt.subplots(figsize=(9, 9))\n",
        "for module_url in module_urls:\n",
        "      print(module_url)\n",
        "      df = pd.read_csv(name_dataset, delimiter='\\t')\n",
        "      if module_url == 'tfidf':\n",
        "        print('tfidf')\n",
        "        data =  df['text']\n",
        "        tf_idf_vectorizor =  TfidfVectorizer(max_features = 20)\n",
        "        tf_idf = tf_idf_vectorizor.fit_transform( data)\n",
        "        tf_idf_norm =  normalize(tf_idf)\n",
        "        features = tf_idf_norm.toarray()\n",
        "      else:\n",
        "        features_file = f'{path}{module_url}_features_batch_{batch}.txt'\n",
        "        with open(features_file, \"rb\") as f:    \n",
        "            features = np.loadtxt(f)\n",
        "      len_features = len(features)\n",
        "      labels = df['labels'][:len_features]\n",
        "      labels = label_binarize(labels, classes=['качество','конфиденциальность','обязанности сторон','оплат','ответственность сторон','предмет договора','приёмка товара', 'прочие условия', 'споров', 'срок действия договора','сроки поставки', 'форс-мажор' ])\n",
        "      n_classes = labels.shape[1]\n",
        "      train_features, test_features, train_labels, test_labels = train_test_split(features, labels, test_size=0.9)\n",
        "      names = ['OVR_RandomForestClassifier']\n",
        "      classifiers = [OneVsRestClassifier(RandomForestClassifier())]\n",
        "      for name, clf in zip(names, classifiers):\n",
        "          t0 = time.time() \n",
        "          #clf.fit(train_features, train_labels) \n",
        "          path_algoritm = f'{path}/{module_url}_{name}'\n",
        "          # with open (path_algoritm, 'wb') as f:\n",
        "          #     pickle.dump(clf, f)    \n",
        "          with open (path_algoritm, 'rb') as f:\n",
        "              clf =  pickle.load(f)\n",
        "          predictions = clf.predict(test_features)        \n",
        "          y_score = clf.predict_proba(test_features)          \n",
        "          # Построение precision_recall_curve\n",
        "          precision = dict()\n",
        "          recall = dict()\n",
        "          average_precision = dict()\n",
        "          for i in range(n_classes):\n",
        "              precision[i], recall[i], _ = precision_recall_curve(test_labels[:, i],y_score[:, i])\n",
        "              average_precision[i] = average_precision_score(test_labels[:, i], y_score[:, i])\n",
        "          # A \"micro-average\": quantifying score on all classes jointly\n",
        "          precision[\"micro\"], recall[\"micro\"], _ = precision_recall_curve(test_labels.ravel(),y_score.ravel())\n",
        "          average_precision[\"micro\"] = average_precision_score(test_labels, y_score, average=\"micro\")\n",
        "          print('Average precision score, micro-averaged over all classes: {0:0.2f}'.format(average_precision[\"micro\"]))\n",
        "         \n",
        "          plt.step(recall['micro'], precision['micro'], where='post', label= '%s  Average precision score (%0.2f)' % (module_url, average_precision[\"micro\"]))\n",
        "\n",
        "plt.xlabel('Recall')\n",
        "plt.ylabel('Precision')\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.legend()\n",
        "plt.title('{} Average precision score'.format(name))\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9eqJLxbTdgqa"
      },
      "source": [
        "## ROC_CURVE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KO2vZ7IvW3iU"
      },
      "source": [
        "#path = '/content/'   # путь к папке\n",
        "# path = '/content/drive/MyDrive/MODELS/create_embed/'\n",
        "# batch = 80 \n",
        "# name_dataset = f'{path}contract_labels-15000.tsv'\n",
        "#['Metric (weighted)'] = ['precision', 'recall', 'f1']\n",
        "fig, axs = plt.subplots(figsize=(9, 9))\n",
        "for module_url in module_urls:\n",
        "      print(module_url)\n",
        "      df = pd.read_csv(name_dataset, delimiter='\\t')\n",
        "      if module_url == 'tfidf':\n",
        "          print('tfidf')\n",
        "          data =  df['text']\n",
        "          tf_idf_vectorizor =  TfidfVectorizer(max_features = 20)\n",
        "          tf_idf = tf_idf_vectorizor.fit_transform( data)\n",
        "          tf_idf_norm =  normalize(tf_idf)\n",
        "          features = tf_idf_norm.toarray()\n",
        "      else:\n",
        "          features_file = f'{path}{module_url}_features_batch_{batch}.txt'\n",
        "          with open(features_file, \"rb\") as f:    \n",
        "              features = np.loadtxt(f)\n",
        "      len_features = len(features)          \n",
        "      labels = df['labels'][:len_features]          #\n",
        "      labels = label_binarize(labels, classes=['качество', 'конфиденциальность','обязанности сторон','оплат', 'ответственность сторон', 'предмет договора', 'приёмка товара', 'прочие условия', 'споров', 'срок действия договора','сроки поставки', 'форс-мажор' ])\n",
        "      n_classes = labels.shape[1]         \n",
        "      train_features, test_features, train_labels, test_labels = train_test_split(features, labels)\n",
        "      names = ['OVR_RandomForestClassifier']         \n",
        "      classifiers = [\n",
        "          OneVsRestClassifier(RandomForestClassifier())\n",
        "          ]\n",
        "      for name, clf in zip(names, classifiers):\n",
        "          t0 = time.time() \n",
        "          #clf.fit(train_features, train_labels) \n",
        "          path_algoritm = f'{path}/{module_url}_{name}'\n",
        "          # with open (path_algoritm, 'wb') as f:\n",
        "          #     pickle.dump(clf, f)    \n",
        "          with open (path_algoritm, 'rb') as f:\n",
        "              clf =  pickle.load(f)\n",
        "          predictions = clf.predict(test_features)              \n",
        "          accuracy_score = metrics.accuracy_score(test_labels, predictions)                 \n",
        "          #y_score = clf.decision_function(test_features)   \n",
        "          y_score = clf.predict_proba(test_features)\n",
        "          fpr = dict()\n",
        "          tpr = dict()\n",
        "          roc_auc = dict()\n",
        "          for i in range(n_classes):\n",
        "              fpr[i], tpr[i], _ = roc_curve(test_labels[:, i], y_score[:, i])\n",
        "              roc_auc[i] = auc(fpr[i], tpr[i])\n",
        "          # Compute micro-average ROC curve and ROC area\n",
        "          fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(test_labels.ravel(), y_score.ravel())\n",
        "          roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
        "          lw = 2\n",
        "          plt.plot(fpr[\"micro\"], tpr[\"micro\"], lw=lw, label='%s ROC curve  (area = %0.2f)' % (module_url,roc_auc['micro']))\n",
        "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
        "#plt.figure()\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title(f'ROC curve: {name}')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L12XwtOpFoWd"
      },
      "source": [
        "#  Clusterization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UAfpEjlRF3oR"
      },
      "source": [
        "batch = 80\n",
        "path = '/content/drive/MyDrive/MODELS/create_embed/'\n",
        "#path = '/content/'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JSBJkwlEGHOz"
      },
      "source": [
        "module_url = \"laser\" #@param [\"legal_ru_electra\", \"legal_ru_longformer\", \"legal_ru_bert\", \"rubert\", \"muse\", \"laser\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "agO57AnwF22b"
      },
      "source": [
        "features_file = f'{path}{module_url}_features_batch_{batch}.txt'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ZtNIPHLFvIJ"
      },
      "source": [
        "with open(features_file, \"rb\") as f:    \n",
        "    features = np.loadtxt(f)\n",
        "points = np.vstack(features)\n",
        "print(points)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "__cTeKA9qB2Y"
      },
      "source": [
        "name_dataset = f'{path}contract_labels-15000.tsv'\n",
        "df = pd.read_csv(name_dataset, delimiter='\\t')\n",
        "data =  df['text']\n",
        "tf_idf_vectorizor =  TfidfVectorizer(max_features = 20)\n",
        "tf_idf = tf_idf_vectorizor.fit_transform( data)\n",
        "tf_idf_norm =  normalize(tf_idf)\n",
        "points = tf_idf_norm.toarray()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z3Fmgf62dRi1"
      },
      "source": [
        "from sklearn.cluster import KMeans\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import PCA\n",
        "sklearn_pca =  PCA( n_components =  2)\n",
        "Y_sklearn = sklearn_pca.fit_transform(points)\n",
        "kmeans = KMeans(n_clusters=11, max_iter=600, algorithm = 'auto')\n",
        "fitted = kmeans.fit(Y_sklearn)\n",
        "prediction =  kmeans.predict( Y_sklearn)\n",
        "centers =  kmeans.cluster_centers_\n",
        "plt.scatter(Y_sklearn[:, 0], Y_sklearn[:, 1 ], c= prediction, s= 50, cmap= 'viridis')\n",
        "plt.scatter(centers[:, 0], centers[:, 1], c = 'black', s = 200, alpha = 0.5);\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SWFSD9ZJ_qZR"
      },
      "source": [
        "# Testing Machine Learning Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tbhooRQLFdLy"
      },
      "source": [
        "## Translation from ordinary language to legal language"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RCphdXdjHUlC"
      },
      "source": [
        "!pip install flair"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M5D2xfFyH0AP"
      },
      "source": [
        "!pip install sentence_transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S5JzXKVZFZ8F"
      },
      "source": [
        "from sentence_transformers import SentenceTransformer, models\n",
        "model_1 = SentenceTransformer('DeepPavlov/rubert-base-cased-sentence')\n",
        "word_embedding_model = models.Transformer('/content/legal_ru_bert', max_seq_length=758)\n",
        "pooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension())\n",
        "model_2 = SentenceTransformer(modules=[word_embedding_model, pooling_model])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fao8oXlf_pvY"
      },
      "source": [
        "import bokeh\n",
        "import bokeh.models\n",
        "import bokeh.plotting\n",
        "import numpy as np\n",
        "import os\n",
        "import pandas as pd\n",
        "import tensorflow.compat.v2 as tf\n",
        "import tensorflow_hub as hub\n",
        "from tensorflow_text import SentencepieceTokenizer\n",
        "import sklearn.metrics.pairwise\n",
        "\n",
        "from simpleneighbors import SimpleNeighbors\n",
        "from tqdm import tqdm\n",
        "from tqdm import trange\n",
        "\n",
        "def visualize_similarity(embeddings_1, embeddings_2, labels_1, labels_2,\n",
        "                         plot_title,\n",
        "                         plot_width=1200, plot_height=600,\n",
        "                         xaxis_font_size='12pt', yaxis_font_size='12pt'):\n",
        "\n",
        "  assert len(embeddings_1) == len(labels_1)\n",
        "  assert len(embeddings_2) == len(labels_2)\n",
        "\n",
        "  # arccos based text similarity (Yang et al. 2019; Cer et al. 2019)\n",
        "  sim = 1 - np.arccos(\n",
        "      sklearn.metrics.pairwise.cosine_similarity(embeddings_1,\n",
        "                                                 embeddings_2))/np.pi\n",
        "\n",
        "  embeddings_1_col, embeddings_2_col, sim_col = [], [], []\n",
        "  for i in range(len(embeddings_1)):\n",
        "    for j in range(len(embeddings_2)):\n",
        "      embeddings_1_col.append(labels_1[i])\n",
        "      embeddings_2_col.append(labels_2[j])\n",
        "      sim_col.append(sim[i][j])\n",
        "  df = pd.DataFrame(zip(embeddings_1_col, embeddings_2_col, sim_col),\n",
        "                    columns=['embeddings_1', 'embeddings_2', 'sim'])\n",
        "\n",
        "  mapper = bokeh.models.LinearColorMapper(\n",
        "      palette=[*reversed(bokeh.palettes.YlOrRd[9])], low=df.sim.min(),\n",
        "      high=df.sim.max())\n",
        "\n",
        "  p = bokeh.plotting.figure(title=plot_title, x_range=labels_1,\n",
        "                            x_axis_location=\"above\",\n",
        "                            y_range=[*reversed(labels_2)],\n",
        "                            plot_width=plot_width, plot_height=plot_height,\n",
        "                            tools=\"save\",toolbar_location='below', tooltips=[\n",
        "                                ('pair', '@embeddings_1 ||| @embeddings_2'),\n",
        "                                ('sim', '@sim')])\n",
        "  p.rect(x=\"embeddings_1\", y=\"embeddings_2\", width=1, height=1, source=df,\n",
        "         fill_color={'field': 'sim', 'transform': mapper}, line_color=None)\n",
        "\n",
        "  p.title.text_font_size = '12pt'\n",
        "  p.axis.axis_line_color = None\n",
        "  p.axis.major_tick_line_color = None\n",
        "  p.axis.major_label_standoff = 16\n",
        "  p.xaxis.major_label_text_font_size = xaxis_font_size\n",
        "  p.xaxis.major_label_orientation = 0.25 * np.pi\n",
        "  p.yaxis.major_label_text_font_size = yaxis_font_size\n",
        "  p.min_border_right = 300\n",
        "\n",
        "  bokeh.io.output_notebook()\n",
        "  bokeh.io.show(p)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lyEr5VT6AYkJ"
      },
      "source": [
        "legal_example_1 = [\"акция\", \"кредит\", \"вы должны деньги\"]\n",
        "example_2 =  [\"Акционная ценная бумага\", \"кредитный договор\", \"задолжность по договору\" ]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mn1LB5t0Gxpz"
      },
      "source": [
        "embeddings1 = model_1.encode(legal_example_1)\n",
        "embeddings2 = model_2.encode(example_2)\n",
        "embedding_1 = [i for i in embeddings1]\n",
        "embedding_2 = [i for i in embeddings2]\n",
        "print(embeddings1)\n",
        "visualize_similarity(embedding_1, embedding_2, legal_example_1, example_2,  \"Legal_Ru_Bert и Deeppavlov/Rubert\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4RwY-ejmISwZ"
      },
      "source": [
        "embeddings1 = model_1.encode(legal_example_1)\n",
        "embeddings2 = model_1.encode(example_2)\n",
        "embedding_1 = [i for i in embeddings1]\n",
        "embedding_2 = [i for i in embeddings2]\n",
        "print(embeddings1)\n",
        "visualize_similarity(embedding_1, embedding_2, legal_example_1, example_2,  \"Deeppavlov/Rubert\") "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wH662hHiIUYD"
      },
      "source": [
        "embeddings1 = model_2.encode(legal_example_1)\n",
        "embeddings2 = model_2.encode(example_2)\n",
        "embedding_1 = [i for i in embeddings1]\n",
        "embedding_2 = [i for i in embeddings2]\n",
        "print(embeddings1)\n",
        "visualize_similarity(embedding_1, embedding_2, legal_example_1, example_2,  \"Legal_Ru_Bert\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SY00gX4G-Dxp"
      },
      "source": [
        "## Сlassification paragraph of the contract"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g6DsuGhI-DHg"
      },
      "source": [
        "!pip install nltk\n",
        "!pip install python-docx "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cZZQckth96HJ"
      },
      "source": [
        "import pandas as pd\n",
        "import posixpath\n",
        "import urllib.parse\n",
        "import docx\n",
        "import os     \n",
        "import pandas as pd\n",
        "import csv\n",
        "import subprocess\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import brown\n",
        "from nltk.stem.snowball import SnowballStemmer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zWyB_mzm8Clj"
      },
      "source": [
        "id = '1gj1qQItYl_6E5ndXQa-YbBZ17oKBmT37'\n",
        "!wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=$id' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=$id\" -O Договор_поставки.zip && rm -rf /tmp/cookies.txt\n",
        "!unzip -uq Договор_поставки.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EVSt4ZVA-Y1x"
      },
      "source": [
        "from docx import Document  \n",
        "import pprint\n",
        "def func():\n",
        "    mydict = {}\n",
        "    doc = docx.Document('/content/Договор поставки.docx')       \n",
        "    bold = []\n",
        "    pattern = [\"предмет договора\", 'обязанности сторон', 'оплат', 'цен','споров','форс-мажор','обстоятельства непреодолимой силы','приёмка товара','прочие условия','срок действия договора','качество', 'сроки поставки', 'ответственность сторон','конфиденциальность']\n",
        "    all = '' \n",
        "    for para in doc.paragraphs:    \n",
        "        for r in para.runs:\n",
        "            if r.font.bold:            \n",
        "                bold.append(r.text)\n",
        "            if r.font.italic:           \n",
        "                bold.append(r.text)        \n",
        "    for para in doc.paragraphs:     \n",
        "        all +=para.text\n",
        "    index_start_text = [] \n",
        "    index_end_text = [] \n",
        "    head_pattern = []\n",
        "    titles = []\n",
        "    for title in bold:\n",
        "      for p in pattern:\n",
        "        if title.lower().find(p) != -1 :\n",
        "          head_pattern.append(p)\n",
        "          titles.append(title)        \n",
        "          start_title = all.find(title)\n",
        "          end_title = start_title+len(title)        \n",
        "          index_start_text.append(end_title)       \n",
        "    index_start_text.append(index_start_text[len(index_start_text)-1]+100)\n",
        "    len_index_start_text = len(index_start_text)\n",
        "    try:\n",
        "      for title, ind_start_text, head in zip(titles, index_start_text, head_pattern):\n",
        "        print('title ', title)        \n",
        "        find_next_index_in_list= index_start_text.index(ind_start_text)+1\n",
        "        index_end_text = index_start_text[find_next_index_in_list]-len(title)\n",
        "        text_parag = all[ind_start_text:index_end_text]\n",
        "        sent = sent_tokenize(text_parag)            \n",
        "        mydict[head] = text_parag[5:1000]\n",
        "    except Exception as e:\n",
        "      print(e)  \n",
        "    return mydict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NUKu-6TuCG6p"
      },
      "source": [
        "mydict = func()\n",
        "df = pd.DataFrame.from_dict(mydict, orient='index')\n",
        "df = df.transpose()\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oLXp44kDGThd"
      },
      "source": [
        "module_url = \"legal_ru_electra\" #@param [\"legal_ru_electra\", \"legal_ru_longformer\", \"legal_ru_bert\", \"DeepPavlov/rubert-base-cased\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mkmF3w8WGLcM"
      },
      "source": [
        "model_type = ElectraModel #@param [\"ElectraModel\", \"AutoModel\", \"LongformerModel\", \"BertModel\"] {type:\"raw\", allow-input: true}\n",
        "model = model_type.from_pretrained(module_url) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TKPdAh58FJh0"
      },
      "source": [
        "tokenizer_type = ElectraTokenizerFast #@param [\"AutoTokenizer\", \"ElectraTokenizerFast\", \"LongformerTokenizer\", \"BertTokenizer\"] {type:\"raw\", allow-input: true}\n",
        "tokenizer = tokenizer_type.from_pretrained(module_url)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ql9nqKBrH0e6"
      },
      "source": [
        "#path = '/content/'   # путь к папке\n",
        "path = '/content/drive/MyDrive/MODELS/create_embed'\n",
        "path_algoritm = f'{path}/{module_url}_OVR_RandomForestClassifier'\n",
        "with open (path_algoritm, 'rb') as f:\n",
        "      clf =  pickle.load(f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YDAF2zUbuENB"
      },
      "source": [
        "mydict.items()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zEdmDBlSuMK5"
      },
      "source": [
        "df['предмет договора']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eh5Jahq0dUKM"
      },
      "source": [
        "name_dataset = f'{path}/contract_labels-15000.tsv'\n",
        "df = pd.read_csv(name_dataset, delimiter='\\t')\n",
        "labels = df['labels']\n",
        "labels_bin = label_binarize(labels, classes=['качество', 'конфиденциальность','обязанности сторон','оплат', 'ответственность сторон', 'предмет договора', 'приёмка товара', 'прочие условия', 'споров', 'срок действия договора','сроки поставки', 'форс-мажор' ])\n",
        "\n",
        "df.groupby(['labels'])['labels'].count().plot(kind='bar')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-IkhVdjAPS2E"
      },
      "source": [
        "classes_labels=['качество', 'конфиденциальность','обязанности сторон','оплат', 'ответственность сторон', 'предмет договора', 'приёмка товара', 'прочие условия', 'споров', 'срок действия договора','сроки поставки', 'форс-мажор' ]\n",
        "for label, sent in mydict.items():\n",
        "  d = {} \n",
        "  tokenized = df[label].apply(lambda x: tokenizer.encode(x, add_special_tokens=True))  \n",
        "  max_len = 0\n",
        "  for i in tokenized.values:\n",
        "      if len(i) > max_len:\n",
        "          max_len = len(i)\n",
        "  padded = np.array([i + [0]*(max_len-len(i)) for i in tokenized.values])              \n",
        "  attention_mask = np.where(padded != 0, 1, 0)\n",
        "  input_ids = torch.tensor(padded) \n",
        "  attention_mask = torch.tensor(attention_mask)              \n",
        "  with torch.no_grad(): \n",
        "    last_hidden_states = model(input_ids, attention_mask=attention_mask)       \n",
        "    features = last_hidden_states[0][:,0,:].numpy()       \n",
        "    predictions = clf.predict_proba(features)\n",
        "    del features    \n",
        "    prediction_list = predictions[0].tolist()\n",
        "    for pred, l in zip(prediction_list, classes_labels):     \n",
        "      d[l]=pred\n",
        "  print(sent) \n",
        "  print() \n",
        "  df1 = pd.DataFrame.from_dict(d, orient='index')\n",
        "  df1.plot(kind='bar')  \n",
        "  plt.show\n",
        "  print('True_labels: ', label)\n",
        "  print('_______________')   "
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}